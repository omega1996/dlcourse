{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[[2, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25455.182912\n",
      "Epoch 1, loss: 26928.310630\n",
      "Epoch 2, loss: 26611.885157\n",
      "Epoch 3, loss: 26995.425987\n",
      "Epoch 4, loss: 27096.880776\n",
      "Epoch 5, loss: 26210.793847\n",
      "Epoch 6, loss: 26911.822848\n",
      "Epoch 7, loss: 25936.854204\n",
      "Epoch 8, loss: 24215.358505\n",
      "Epoch 9, loss: 26642.289629\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f99913c86d0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 25994.650562\n",
      "Epoch 1, loss: 26920.096406\n",
      "Epoch 2, loss: 25213.456542\n",
      "Epoch 3, loss: 28049.569628\n",
      "Epoch 4, loss: 26689.940869\n",
      "Epoch 5, loss: 25386.348590\n",
      "Epoch 6, loss: 25039.112087\n",
      "Epoch 7, loss: 27265.390094\n",
      "Epoch 8, loss: 27256.802332\n",
      "Epoch 9, loss: 28062.113944\n",
      "Epoch 10, loss: 26052.925354\n",
      "Epoch 11, loss: 26209.660907\n",
      "Epoch 12, loss: 27484.413573\n",
      "Epoch 13, loss: 27446.307671\n",
      "Epoch 14, loss: 25695.665638\n",
      "Epoch 15, loss: 26972.530221\n",
      "Epoch 16, loss: 25808.123091\n",
      "Epoch 17, loss: 25296.899747\n",
      "Epoch 18, loss: 27734.807150\n",
      "Epoch 19, loss: 27202.949737\n",
      "Epoch 20, loss: 25885.015066\n",
      "Epoch 21, loss: 26735.523605\n",
      "Epoch 22, loss: 26820.567161\n",
      "Epoch 23, loss: 25807.412469\n",
      "Epoch 24, loss: 26162.520955\n",
      "Epoch 25, loss: 26714.836530\n",
      "Epoch 26, loss: 27056.775451\n",
      "Epoch 27, loss: 25838.665197\n",
      "Epoch 28, loss: 27364.895207\n",
      "Epoch 29, loss: 25736.090354\n",
      "Epoch 30, loss: 27222.549582\n",
      "Epoch 31, loss: 26052.654146\n",
      "Epoch 32, loss: 26390.551944\n",
      "Epoch 33, loss: 28193.099629\n",
      "Epoch 34, loss: 25491.865036\n",
      "Epoch 35, loss: 27020.827638\n",
      "Epoch 36, loss: 26977.541775\n",
      "Epoch 37, loss: 26964.163076\n",
      "Epoch 38, loss: 27282.343140\n",
      "Epoch 39, loss: 26369.870117\n",
      "Epoch 40, loss: 25530.419421\n",
      "Epoch 41, loss: 27060.582040\n",
      "Epoch 42, loss: 26776.341415\n",
      "Epoch 43, loss: 25614.341673\n",
      "Epoch 44, loss: 27358.130328\n",
      "Epoch 45, loss: 25605.823761\n",
      "Epoch 46, loss: 27810.900971\n",
      "Epoch 47, loss: 26493.566082\n",
      "Epoch 48, loss: 26388.393620\n",
      "Epoch 49, loss: 25520.294039\n",
      "Epoch 50, loss: 27064.804272\n",
      "Epoch 51, loss: 26968.221314\n",
      "Epoch 52, loss: 26954.209908\n",
      "Epoch 53, loss: 24802.844362\n",
      "Epoch 54, loss: 26525.732444\n",
      "Epoch 55, loss: 28438.182437\n",
      "Epoch 56, loss: 26451.868339\n",
      "Epoch 57, loss: 26486.399173\n",
      "Epoch 58, loss: 26336.782827\n",
      "Epoch 59, loss: 27071.340968\n",
      "Epoch 60, loss: 26172.600321\n",
      "Epoch 61, loss: 26981.620996\n",
      "Epoch 62, loss: 26127.787349\n",
      "Epoch 63, loss: 27188.068511\n",
      "Epoch 64, loss: 27012.378910\n",
      "Epoch 65, loss: 28477.654860\n",
      "Epoch 66, loss: 26764.645559\n",
      "Epoch 67, loss: 27648.616371\n",
      "Epoch 68, loss: 26687.730673\n",
      "Epoch 69, loss: 27000.891124\n",
      "Epoch 70, loss: 28418.918113\n",
      "Epoch 71, loss: 25861.563560\n",
      "Epoch 72, loss: 27630.810500\n",
      "Epoch 73, loss: 26628.952418\n",
      "Epoch 74, loss: 25846.855812\n",
      "Epoch 75, loss: 27567.532691\n",
      "Epoch 76, loss: 25647.270618\n",
      "Epoch 77, loss: 28231.170722\n",
      "Epoch 78, loss: 25472.211083\n",
      "Epoch 79, loss: 26639.484144\n",
      "Epoch 80, loss: 27040.179769\n",
      "Epoch 81, loss: 27023.111153\n",
      "Epoch 82, loss: 26397.094174\n",
      "Epoch 83, loss: 26878.474041\n",
      "Epoch 84, loss: 26307.892750\n",
      "Epoch 85, loss: 26031.384616\n",
      "Epoch 86, loss: 27118.470799\n",
      "Epoch 87, loss: 26557.776621\n",
      "Epoch 88, loss: 25649.050311\n",
      "Epoch 89, loss: 28264.967182\n",
      "Epoch 90, loss: 26047.917391\n",
      "Epoch 91, loss: 27198.897003\n",
      "Epoch 92, loss: 26508.822110\n",
      "Epoch 93, loss: 26476.710301\n",
      "Epoch 94, loss: 26580.695294\n",
      "Epoch 95, loss: 27316.583802\n",
      "Epoch 96, loss: 27275.455629\n",
      "Epoch 97, loss: 28034.267089\n",
      "Epoch 98, loss: 25086.757943\n",
      "Epoch 99, loss: 27637.477183\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25759.592558\n",
      "Epoch 1, loss: 25460.946816\n",
      "Epoch 2, loss: 25151.515980\n",
      "Epoch 3, loss: 25739.526796\n",
      "Epoch 4, loss: 25623.892221\n",
      "Epoch 5, loss: 24492.844342\n",
      "Epoch 6, loss: 24865.140192\n",
      "Epoch 7, loss: 24484.315318\n",
      "Epoch 8, loss: 25725.197654\n",
      "Epoch 9, loss: 23592.384122\n",
      "Epoch 10, loss: 24061.580241\n",
      "Epoch 11, loss: 26129.167136\n",
      "Epoch 12, loss: 23797.954559\n",
      "Epoch 13, loss: 23698.613272\n",
      "Epoch 14, loss: 24952.172215\n",
      "Epoch 15, loss: 24722.113038\n",
      "Epoch 16, loss: 23558.893476\n",
      "Epoch 17, loss: 23934.593421\n",
      "Epoch 18, loss: 24645.247478\n",
      "Epoch 19, loss: 24968.534430\n",
      "Epoch 20, loss: 23108.452481\n",
      "Epoch 21, loss: 23279.041648\n",
      "Epoch 22, loss: 24038.211386\n",
      "Epoch 23, loss: 24041.039260\n",
      "Epoch 24, loss: 24339.125064\n",
      "Epoch 25, loss: 24752.907949\n",
      "Epoch 26, loss: 23670.110499\n",
      "Epoch 27, loss: 24364.829429\n",
      "Epoch 28, loss: 24665.140888\n",
      "Epoch 29, loss: 24334.846817\n",
      "Epoch 30, loss: 24086.028118\n",
      "Epoch 31, loss: 23117.325172\n",
      "Epoch 32, loss: 24218.457961\n",
      "Epoch 33, loss: 23122.599286\n",
      "Epoch 34, loss: 23754.669107\n",
      "Epoch 35, loss: 23861.991387\n",
      "Epoch 36, loss: 24259.116497\n",
      "Epoch 37, loss: 23567.098392\n",
      "Epoch 38, loss: 24497.221617\n",
      "Epoch 39, loss: 24346.689337\n",
      "Epoch 40, loss: 23272.153696\n",
      "Epoch 41, loss: 23828.587491\n",
      "Epoch 42, loss: 23955.513989\n",
      "Epoch 43, loss: 23271.407068\n",
      "Epoch 44, loss: 23797.635817\n",
      "Epoch 45, loss: 23436.620855\n",
      "Epoch 46, loss: 23810.138567\n",
      "Epoch 47, loss: 23562.291529\n",
      "Epoch 48, loss: 24503.162555\n",
      "Epoch 49, loss: 23540.983022\n",
      "Epoch 50, loss: 22671.272809\n",
      "Epoch 51, loss: 23364.521158\n",
      "Epoch 52, loss: 23110.950656\n",
      "Epoch 53, loss: 24268.286160\n",
      "Epoch 54, loss: 23001.078302\n",
      "Epoch 55, loss: 24398.584890\n",
      "Epoch 56, loss: 23377.321802\n",
      "Epoch 57, loss: 23494.349912\n",
      "Epoch 58, loss: 23233.217106\n",
      "Epoch 59, loss: 23455.651737\n",
      "Epoch 60, loss: 22143.896644\n",
      "Epoch 61, loss: 24088.657010\n",
      "Epoch 62, loss: 24013.892034\n",
      "Epoch 63, loss: 22551.754186\n",
      "Epoch 64, loss: 22879.207994\n",
      "Epoch 65, loss: 22946.186226\n",
      "Epoch 66, loss: 22713.095746\n",
      "Epoch 67, loss: 23231.673018\n",
      "Epoch 68, loss: 24241.135871\n",
      "Epoch 69, loss: 22367.630792\n",
      "Epoch 70, loss: 24028.254038\n",
      "Epoch 71, loss: 23833.002374\n",
      "Epoch 72, loss: 22079.083349\n",
      "Epoch 73, loss: 23717.548894\n",
      "Epoch 74, loss: 21858.704471\n",
      "Epoch 75, loss: 24321.830567\n",
      "Epoch 76, loss: 22980.940759\n",
      "Epoch 77, loss: 22340.124801\n",
      "Epoch 78, loss: 23007.571991\n",
      "Epoch 79, loss: 23524.902328\n",
      "Epoch 80, loss: 23123.862204\n",
      "Epoch 81, loss: 23305.433094\n",
      "Epoch 82, loss: 23639.821026\n",
      "Epoch 83, loss: 21942.425886\n",
      "Epoch 84, loss: 24395.666979\n",
      "Epoch 85, loss: 21338.592760\n",
      "Epoch 86, loss: 23388.283021\n",
      "Epoch 87, loss: 22999.426548\n",
      "Epoch 88, loss: 23126.923213\n",
      "Epoch 89, loss: 22122.488632\n",
      "Epoch 90, loss: 22642.295646\n",
      "Epoch 91, loss: 23578.850045\n",
      "Epoch 92, loss: 23956.248242\n",
      "Epoch 93, loss: 21808.752996\n",
      "Epoch 94, loss: 24122.833313\n",
      "Epoch 95, loss: 23642.850467\n",
      "Epoch 96, loss: 24329.602846\n",
      "Epoch 97, loss: 21302.100479\n",
      "Epoch 98, loss: 23539.829124\n",
      "Epoch 99, loss: 23835.801982\n",
      "Epoch 100, loss: 23465.881873\n",
      "Epoch 101, loss: 22299.862288\n",
      "Epoch 102, loss: 22316.319600\n",
      "Epoch 103, loss: 24052.932438\n",
      "Epoch 104, loss: 22997.314326\n",
      "Epoch 105, loss: 22902.683393\n",
      "Epoch 106, loss: 22344.928269\n",
      "Epoch 107, loss: 23201.463982\n",
      "Epoch 108, loss: 23432.160926\n",
      "Epoch 109, loss: 22796.152697\n",
      "Epoch 110, loss: 23124.565842\n",
      "Epoch 111, loss: 23355.841929\n",
      "Epoch 112, loss: 22841.875351\n",
      "Epoch 113, loss: 22656.877301\n",
      "Epoch 114, loss: 23211.533329\n",
      "Epoch 115, loss: 22500.147139\n",
      "Epoch 116, loss: 22937.391832\n",
      "Epoch 117, loss: 22028.609993\n",
      "Epoch 118, loss: 23420.729570\n",
      "Epoch 119, loss: 23370.252214\n",
      "Epoch 120, loss: 23123.207049\n",
      "Epoch 121, loss: 22169.242213\n",
      "Epoch 122, loss: 23707.704267\n",
      "Epoch 123, loss: 22540.958152\n",
      "Epoch 124, loss: 22405.310931\n",
      "Epoch 125, loss: 21911.518205\n",
      "Epoch 126, loss: 22544.674479\n",
      "Epoch 127, loss: 23482.452960\n",
      "Epoch 128, loss: 23438.004607\n",
      "Epoch 129, loss: 22025.876870\n",
      "Epoch 130, loss: 23550.270055\n",
      "Epoch 131, loss: 22060.451169\n",
      "Epoch 132, loss: 24278.754903\n",
      "Epoch 133, loss: 22617.848205\n",
      "Epoch 134, loss: 21587.939563\n",
      "Epoch 135, loss: 23780.169630\n",
      "Epoch 136, loss: 22773.763863\n",
      "Epoch 137, loss: 22111.445531\n",
      "Epoch 138, loss: 21317.840032\n",
      "Epoch 139, loss: 21973.875464\n",
      "Epoch 140, loss: 23695.672271\n",
      "Epoch 141, loss: 21856.368663\n",
      "Epoch 142, loss: 23458.060244\n",
      "Epoch 143, loss: 21487.576679\n",
      "Epoch 144, loss: 23070.454869\n",
      "Epoch 145, loss: 22409.569904\n",
      "Epoch 146, loss: 22765.207022\n",
      "Epoch 147, loss: 24791.217857\n",
      "Epoch 148, loss: 22284.532845\n",
      "Epoch 149, loss: 21776.569892\n",
      "Epoch 150, loss: 23092.609070\n",
      "Epoch 151, loss: 22768.001677\n",
      "Epoch 152, loss: 22129.911806\n",
      "Epoch 153, loss: 23101.720944\n",
      "Epoch 154, loss: 21851.092609\n",
      "Epoch 155, loss: 23113.137938\n",
      "Epoch 156, loss: 21387.765218\n",
      "Epoch 157, loss: 22269.681384\n",
      "Epoch 158, loss: 23331.565393\n",
      "Epoch 159, loss: 21798.427553\n",
      "Epoch 160, loss: 21758.322520\n",
      "Epoch 161, loss: 23331.954916\n",
      "Epoch 162, loss: 22606.030439\n",
      "Epoch 163, loss: 21127.115209\n",
      "Epoch 164, loss: 22251.533202\n",
      "Epoch 165, loss: 22964.226457\n",
      "Epoch 166, loss: 22147.893624\n",
      "Epoch 167, loss: 22514.801855\n",
      "Epoch 168, loss: 22415.568833\n",
      "Epoch 169, loss: 22999.975024\n",
      "Epoch 170, loss: 22065.540558\n",
      "Epoch 171, loss: 22860.608798\n",
      "Epoch 172, loss: 21669.442468\n",
      "Epoch 173, loss: 23603.128492\n",
      "Epoch 174, loss: 22399.362636\n",
      "Epoch 175, loss: 22560.628072\n",
      "Epoch 176, loss: 21299.289717\n",
      "Epoch 177, loss: 22372.374943\n",
      "Epoch 178, loss: 22220.600439\n",
      "Epoch 179, loss: 22872.618040\n",
      "Epoch 180, loss: 23202.682308\n",
      "Epoch 181, loss: 22152.372194\n",
      "Epoch 182, loss: 22077.882731\n",
      "Epoch 183, loss: 21489.075080\n",
      "Epoch 184, loss: 21996.256694\n",
      "Epoch 185, loss: 21522.736396\n",
      "Epoch 186, loss: 23291.596384\n",
      "Epoch 187, loss: 21270.225830\n",
      "Epoch 188, loss: 23703.691811\n",
      "Epoch 189, loss: 21828.730841\n",
      "Epoch 190, loss: 21308.355172\n",
      "Epoch 191, loss: 21486.853386\n",
      "Epoch 192, loss: 22477.939629\n",
      "Epoch 193, loss: 23044.217800\n",
      "Epoch 194, loss: 22572.432239\n",
      "Epoch 195, loss: 22721.222219\n",
      "Epoch 196, loss: 22440.154302\n",
      "Epoch 197, loss: 21679.176677\n",
      "Epoch 198, loss: 21435.680173\n",
      "Epoch 199, loss: 22661.006221\n",
      "0.001 0.0001 0.187\n",
      "Epoch 0, loss: 25461.515065\n",
      "Epoch 1, loss: 26584.857078\n",
      "Epoch 2, loss: 25826.549321\n",
      "Epoch 3, loss: 26218.576884\n",
      "Epoch 4, loss: 26595.993973\n",
      "Epoch 5, loss: 23740.460647\n",
      "Epoch 6, loss: 25240.034029\n",
      "Epoch 7, loss: 24229.306433\n",
      "Epoch 8, loss: 25126.515093\n",
      "Epoch 9, loss: 25721.594504\n",
      "Epoch 10, loss: 24530.571213\n",
      "Epoch 11, loss: 23922.127050\n",
      "Epoch 12, loss: 24913.597792\n",
      "Epoch 13, loss: 23888.322290\n",
      "Epoch 14, loss: 24671.915608\n",
      "Epoch 15, loss: 24309.222811\n",
      "Epoch 16, loss: 25001.785951\n",
      "Epoch 17, loss: 24241.513146\n",
      "Epoch 18, loss: 25653.162434\n",
      "Epoch 19, loss: 24148.001181\n",
      "Epoch 20, loss: 23946.475316\n",
      "Epoch 21, loss: 25126.706720\n",
      "Epoch 22, loss: 23765.146125\n",
      "Epoch 23, loss: 24198.145039\n",
      "Epoch 24, loss: 23423.971208\n",
      "Epoch 25, loss: 24279.631251\n",
      "Epoch 26, loss: 23006.084066\n",
      "Epoch 27, loss: 23492.871335\n",
      "Epoch 28, loss: 25946.961738\n",
      "Epoch 29, loss: 24128.470096\n",
      "Epoch 30, loss: 25127.840078\n",
      "Epoch 31, loss: 23703.617885\n",
      "Epoch 32, loss: 24447.348960\n",
      "Epoch 33, loss: 23473.013222\n",
      "Epoch 34, loss: 23600.101083\n",
      "Epoch 35, loss: 23336.791773\n",
      "Epoch 36, loss: 22777.742999\n",
      "Epoch 37, loss: 21719.069073\n",
      "Epoch 38, loss: 24877.467489\n",
      "Epoch 39, loss: 23781.120273\n",
      "Epoch 40, loss: 24499.745388\n",
      "Epoch 41, loss: 23994.115301\n",
      "Epoch 42, loss: 22877.241138\n",
      "Epoch 43, loss: 24283.858727\n",
      "Epoch 44, loss: 24237.860589\n",
      "Epoch 45, loss: 23547.760543\n",
      "Epoch 46, loss: 23843.522976\n",
      "Epoch 47, loss: 24469.892589\n",
      "Epoch 48, loss: 22357.518648\n",
      "Epoch 49, loss: 23526.448330\n",
      "Epoch 50, loss: 23289.235787\n",
      "Epoch 51, loss: 22819.601053\n",
      "Epoch 52, loss: 24226.141508\n",
      "Epoch 53, loss: 24497.421410\n",
      "Epoch 54, loss: 22477.979853\n",
      "Epoch 55, loss: 24005.851236\n",
      "Epoch 56, loss: 23877.483230\n",
      "Epoch 57, loss: 24488.672797\n",
      "Epoch 58, loss: 23627.656995\n",
      "Epoch 59, loss: 23175.784901\n",
      "Epoch 60, loss: 22672.670254\n",
      "Epoch 61, loss: 23166.092032\n",
      "Epoch 62, loss: 24104.808934\n",
      "Epoch 63, loss: 23183.398619\n",
      "Epoch 64, loss: 24488.389594\n",
      "Epoch 65, loss: 22156.912677\n",
      "Epoch 66, loss: 22321.758458\n",
      "Epoch 67, loss: 24695.714181\n",
      "Epoch 68, loss: 22149.353511\n",
      "Epoch 69, loss: 23969.245919\n",
      "Epoch 70, loss: 23228.615730\n",
      "Epoch 71, loss: 23618.181190\n",
      "Epoch 72, loss: 21660.888493\n",
      "Epoch 73, loss: 22707.503578\n",
      "Epoch 74, loss: 24288.464568\n",
      "Epoch 75, loss: 23279.296711\n",
      "Epoch 76, loss: 23648.869608\n",
      "Epoch 77, loss: 22888.956172\n",
      "Epoch 78, loss: 22839.554022\n",
      "Epoch 79, loss: 23335.634799\n",
      "Epoch 80, loss: 22553.929906\n",
      "Epoch 81, loss: 23053.876223\n",
      "Epoch 82, loss: 23118.220127\n",
      "Epoch 83, loss: 22655.573412\n",
      "Epoch 84, loss: 24240.324176\n",
      "Epoch 85, loss: 23646.835150\n",
      "Epoch 86, loss: 21541.942275\n",
      "Epoch 87, loss: 24746.440206\n",
      "Epoch 88, loss: 22981.770711\n",
      "Epoch 89, loss: 21651.638458\n",
      "Epoch 90, loss: 23445.242805\n",
      "Epoch 91, loss: 21981.157493\n",
      "Epoch 92, loss: 24679.581035\n",
      "Epoch 93, loss: 22829.122291\n",
      "Epoch 94, loss: 22986.733934\n",
      "Epoch 95, loss: 24407.743759\n",
      "Epoch 96, loss: 22141.831393\n",
      "Epoch 97, loss: 22153.829933\n",
      "Epoch 98, loss: 24163.670127\n",
      "Epoch 99, loss: 22899.109047\n",
      "Epoch 100, loss: 23814.504837\n",
      "Epoch 101, loss: 22931.513681\n",
      "Epoch 102, loss: 21371.880859\n",
      "Epoch 103, loss: 22660.708277\n",
      "Epoch 104, loss: 22160.489656\n",
      "Epoch 105, loss: 22466.172849\n",
      "Epoch 106, loss: 22548.639147\n",
      "Epoch 107, loss: 22656.009650\n",
      "Epoch 108, loss: 22870.969768\n",
      "Epoch 109, loss: 22685.621847\n",
      "Epoch 110, loss: 22887.902765\n",
      "Epoch 111, loss: 23952.995662\n",
      "Epoch 112, loss: 21817.728167\n",
      "Epoch 113, loss: 23341.750652\n",
      "Epoch 114, loss: 20968.023149\n",
      "Epoch 115, loss: 23501.442216\n",
      "Epoch 116, loss: 23283.216729\n",
      "Epoch 117, loss: 23013.781297\n",
      "Epoch 118, loss: 22518.269996\n",
      "Epoch 119, loss: 22975.876856\n",
      "Epoch 120, loss: 22615.807517\n",
      "Epoch 121, loss: 23320.017203\n",
      "Epoch 122, loss: 22312.463536\n",
      "Epoch 123, loss: 22570.765239\n",
      "Epoch 124, loss: 21929.797995\n",
      "Epoch 125, loss: 22323.057487\n",
      "Epoch 126, loss: 21758.764337\n",
      "Epoch 127, loss: 22657.668840\n",
      "Epoch 128, loss: 23142.054814\n",
      "Epoch 129, loss: 22121.720060\n",
      "Epoch 130, loss: 21214.096378\n",
      "Epoch 131, loss: 23228.802623\n",
      "Epoch 132, loss: 23085.848671\n",
      "Epoch 133, loss: 22680.331451\n",
      "Epoch 134, loss: 22854.169567\n",
      "Epoch 135, loss: 21690.114644\n",
      "Epoch 136, loss: 23660.812821\n",
      "Epoch 137, loss: 21536.028773\n",
      "Epoch 138, loss: 23865.244259\n",
      "Epoch 139, loss: 24205.008902\n",
      "Epoch 140, loss: 21898.406805\n",
      "Epoch 141, loss: 22781.603775\n",
      "Epoch 142, loss: 21241.772731\n",
      "Epoch 143, loss: 23527.627185\n",
      "Epoch 144, loss: 22701.329120\n",
      "Epoch 145, loss: 22154.966519\n",
      "Epoch 146, loss: 22990.552005\n",
      "Epoch 147, loss: 20538.487182\n",
      "Epoch 148, loss: 23100.584235\n",
      "Epoch 149, loss: 21894.470775\n",
      "Epoch 150, loss: 21738.783417\n",
      "Epoch 151, loss: 22976.749975\n",
      "Epoch 152, loss: 23575.538101\n",
      "Epoch 153, loss: 22705.303313\n",
      "Epoch 154, loss: 21333.281035\n",
      "Epoch 155, loss: 23297.019397\n",
      "Epoch 156, loss: 22947.755111\n",
      "Epoch 157, loss: 22267.225919\n",
      "Epoch 158, loss: 22421.216490\n",
      "Epoch 159, loss: 23339.496404\n",
      "Epoch 160, loss: 21525.516420\n",
      "Epoch 161, loss: 22378.909737\n",
      "Epoch 162, loss: 23007.449873\n",
      "Epoch 163, loss: 22071.720441\n",
      "Epoch 164, loss: 22564.914865\n",
      "Epoch 165, loss: 23510.835692\n",
      "Epoch 166, loss: 22048.265537\n",
      "Epoch 167, loss: 23440.550807\n",
      "Epoch 168, loss: 21473.698120\n",
      "Epoch 169, loss: 21904.128626\n",
      "Epoch 170, loss: 23252.363345\n",
      "Epoch 171, loss: 21215.090794\n",
      "Epoch 172, loss: 21576.231678\n",
      "Epoch 173, loss: 23576.317208\n",
      "Epoch 174, loss: 22940.361294\n",
      "Epoch 175, loss: 22173.505695\n",
      "Epoch 176, loss: 23608.237513\n",
      "Epoch 177, loss: 22136.154941\n",
      "Epoch 178, loss: 21849.995285\n",
      "Epoch 179, loss: 21786.853604\n",
      "Epoch 180, loss: 21500.600894\n",
      "Epoch 181, loss: 22671.720806\n",
      "Epoch 182, loss: 20833.004732\n",
      "Epoch 183, loss: 21091.679040\n",
      "Epoch 184, loss: 21953.126295\n",
      "Epoch 185, loss: 23500.642180\n",
      "Epoch 186, loss: 21042.602933\n",
      "Epoch 187, loss: 22833.959839\n",
      "Epoch 188, loss: 22464.550369\n",
      "Epoch 189, loss: 22002.609233\n",
      "Epoch 190, loss: 19832.154143\n",
      "Epoch 191, loss: 23050.265273\n",
      "Epoch 192, loss: 22585.207058\n",
      "Epoch 193, loss: 22943.659138\n",
      "Epoch 194, loss: 22299.164149\n",
      "Epoch 195, loss: 22693.801540\n",
      "Epoch 196, loss: 22085.783506\n",
      "Epoch 197, loss: 22601.424400\n",
      "Epoch 198, loss: 22344.172610\n",
      "Epoch 199, loss: 21758.125005\n",
      "0.001 1e-05 0.196\n",
      "Epoch 0, loss: 25980.196442\n",
      "Epoch 1, loss: 25349.962750\n",
      "Epoch 2, loss: 24862.114238\n",
      "Epoch 3, loss: 25986.330987\n",
      "Epoch 4, loss: 24776.285261\n",
      "Epoch 5, loss: 26357.458844\n",
      "Epoch 6, loss: 25935.489241\n",
      "Epoch 7, loss: 24975.519630\n",
      "Epoch 8, loss: 24653.281656\n",
      "Epoch 9, loss: 24770.224746\n",
      "Epoch 10, loss: 25559.717162\n",
      "Epoch 11, loss: 23562.908949\n",
      "Epoch 12, loss: 24865.837187\n",
      "Epoch 13, loss: 24572.421174\n",
      "Epoch 14, loss: 23655.060675\n",
      "Epoch 15, loss: 24276.756977\n",
      "Epoch 16, loss: 22492.720070\n",
      "Epoch 17, loss: 26080.478474\n",
      "Epoch 18, loss: 23652.761179\n",
      "Epoch 19, loss: 22313.253986\n",
      "Epoch 20, loss: 25725.107396\n",
      "Epoch 21, loss: 24246.929446\n",
      "Epoch 22, loss: 23739.808192\n",
      "Epoch 23, loss: 26130.308653\n",
      "Epoch 24, loss: 24064.273214\n",
      "Epoch 25, loss: 24160.944649\n",
      "Epoch 26, loss: 26003.591575\n",
      "Epoch 27, loss: 23844.326351\n",
      "Epoch 28, loss: 24408.718510\n",
      "Epoch 29, loss: 22857.205592\n",
      "Epoch 30, loss: 24359.741816\n",
      "Epoch 31, loss: 23214.013608\n",
      "Epoch 32, loss: 23453.335449\n",
      "Epoch 33, loss: 24866.388983\n",
      "Epoch 34, loss: 23345.765076\n",
      "Epoch 35, loss: 23629.371125\n",
      "Epoch 36, loss: 24474.082551\n",
      "Epoch 37, loss: 24168.006973\n",
      "Epoch 38, loss: 23321.295266\n",
      "Epoch 39, loss: 24566.204063\n",
      "Epoch 40, loss: 23488.408618\n",
      "Epoch 41, loss: 23129.834557\n",
      "Epoch 42, loss: 23685.159919\n",
      "Epoch 43, loss: 25232.505380\n",
      "Epoch 44, loss: 23270.905545\n",
      "Epoch 45, loss: 24941.202358\n",
      "Epoch 46, loss: 22536.517955\n",
      "Epoch 47, loss: 24621.809861\n",
      "Epoch 48, loss: 24493.664365\n",
      "Epoch 49, loss: 21664.655507\n",
      "Epoch 50, loss: 23902.591052\n",
      "Epoch 51, loss: 23192.183956\n",
      "Epoch 52, loss: 24147.631541\n",
      "Epoch 53, loss: 24093.295242\n",
      "Epoch 54, loss: 22565.250193\n",
      "Epoch 55, loss: 23841.286284\n",
      "Epoch 56, loss: 24299.696360\n",
      "Epoch 57, loss: 24092.708542\n",
      "Epoch 58, loss: 24635.954887\n",
      "Epoch 59, loss: 24638.381014\n",
      "Epoch 60, loss: 22212.127815\n",
      "Epoch 61, loss: 23370.949866\n",
      "Epoch 62, loss: 22855.527264\n",
      "Epoch 63, loss: 23466.001042\n",
      "Epoch 64, loss: 23869.164651\n",
      "Epoch 65, loss: 23605.325322\n",
      "Epoch 66, loss: 23176.133414\n",
      "Epoch 67, loss: 23133.080316\n",
      "Epoch 68, loss: 23142.131847\n",
      "Epoch 69, loss: 24466.959227\n",
      "Epoch 70, loss: 22984.177498\n",
      "Epoch 71, loss: 21759.959085\n",
      "Epoch 72, loss: 23406.131994\n",
      "Epoch 73, loss: 22634.818767\n",
      "Epoch 74, loss: 23566.340066\n",
      "Epoch 75, loss: 23075.355186\n",
      "Epoch 76, loss: 24020.861955\n",
      "Epoch 77, loss: 23846.327330\n",
      "Epoch 78, loss: 22169.537928\n",
      "Epoch 79, loss: 22881.721347\n",
      "Epoch 80, loss: 23432.059224\n",
      "Epoch 81, loss: 22753.644876\n",
      "Epoch 82, loss: 23909.272923\n",
      "Epoch 83, loss: 23612.407356\n",
      "Epoch 84, loss: 22632.245967\n",
      "Epoch 85, loss: 21769.400769\n",
      "Epoch 86, loss: 22243.850026\n",
      "Epoch 87, loss: 22650.920464\n",
      "Epoch 88, loss: 24150.348178\n",
      "Epoch 89, loss: 23195.827446\n",
      "Epoch 90, loss: 23223.224241\n",
      "Epoch 91, loss: 22825.743808\n",
      "Epoch 92, loss: 22245.335179\n",
      "Epoch 93, loss: 21442.023628\n",
      "Epoch 94, loss: 24077.279360\n",
      "Epoch 95, loss: 22485.447746\n",
      "Epoch 96, loss: 23341.363259\n",
      "Epoch 97, loss: 22844.009904\n",
      "Epoch 98, loss: 23525.651783\n",
      "Epoch 99, loss: 22135.903723\n",
      "Epoch 100, loss: 22549.506988\n",
      "Epoch 101, loss: 23482.263357\n",
      "Epoch 102, loss: 23274.787250\n",
      "Epoch 103, loss: 23240.049448\n",
      "Epoch 104, loss: 22951.546081\n",
      "Epoch 105, loss: 23350.132965\n",
      "Epoch 106, loss: 22519.273545\n",
      "Epoch 107, loss: 22896.711133\n",
      "Epoch 108, loss: 22210.771487\n",
      "Epoch 109, loss: 22928.794676\n",
      "Epoch 110, loss: 21931.988050\n",
      "Epoch 111, loss: 22644.535634\n",
      "Epoch 112, loss: 21653.022514\n",
      "Epoch 113, loss: 22514.327724\n",
      "Epoch 114, loss: 23640.010689\n",
      "Epoch 115, loss: 23117.850665\n",
      "Epoch 116, loss: 22899.466198\n",
      "Epoch 117, loss: 23641.452332\n",
      "Epoch 118, loss: 22139.386926\n",
      "Epoch 119, loss: 23578.901593\n",
      "Epoch 120, loss: 22292.177219\n",
      "Epoch 121, loss: 22022.525284\n",
      "Epoch 122, loss: 21803.279107\n",
      "Epoch 123, loss: 22456.215692\n",
      "Epoch 124, loss: 24040.335203\n",
      "Epoch 125, loss: 22392.120947\n",
      "Epoch 126, loss: 21931.545400\n",
      "Epoch 127, loss: 21757.681880\n",
      "Epoch 128, loss: 21408.417162\n",
      "Epoch 129, loss: 24434.355726\n",
      "Epoch 130, loss: 22516.275201\n",
      "Epoch 131, loss: 23050.402220\n",
      "Epoch 132, loss: 22340.998124\n",
      "Epoch 133, loss: 22580.589354\n",
      "Epoch 134, loss: 22353.536176\n",
      "Epoch 135, loss: 22529.358072\n",
      "Epoch 136, loss: 23646.925371\n",
      "Epoch 137, loss: 21457.288845\n",
      "Epoch 138, loss: 22713.143994\n",
      "Epoch 139, loss: 22963.611190\n",
      "Epoch 140, loss: 22277.112667\n",
      "Epoch 141, loss: 23140.907595\n",
      "Epoch 142, loss: 22073.592779\n",
      "Epoch 143, loss: 23230.988511\n",
      "Epoch 144, loss: 21763.974722\n",
      "Epoch 145, loss: 22364.979940\n",
      "Epoch 146, loss: 22479.944071\n",
      "Epoch 147, loss: 21730.801811\n",
      "Epoch 148, loss: 22312.975279\n",
      "Epoch 149, loss: 22128.864505\n",
      "Epoch 150, loss: 22963.499118\n",
      "Epoch 151, loss: 23315.581974\n",
      "Epoch 152, loss: 22234.614454\n",
      "Epoch 153, loss: 21849.687924\n",
      "Epoch 154, loss: 22980.780423\n",
      "Epoch 155, loss: 22732.667486\n",
      "Epoch 156, loss: 23434.927595\n",
      "Epoch 157, loss: 22236.516668\n",
      "Epoch 158, loss: 24156.375066\n",
      "Epoch 159, loss: 21645.684232\n",
      "Epoch 160, loss: 22501.185285\n",
      "Epoch 161, loss: 22085.052850\n",
      "Epoch 162, loss: 23167.293071\n",
      "Epoch 163, loss: 21907.413108\n",
      "Epoch 164, loss: 20854.544802\n",
      "Epoch 165, loss: 22107.879870\n",
      "Epoch 166, loss: 22427.530325\n",
      "Epoch 167, loss: 20612.170493\n",
      "Epoch 168, loss: 22676.892320\n",
      "Epoch 169, loss: 22558.869466\n",
      "Epoch 170, loss: 22320.233429\n",
      "Epoch 171, loss: 21671.100777\n",
      "Epoch 172, loss: 22768.469086\n",
      "Epoch 173, loss: 22021.178490\n",
      "Epoch 174, loss: 21840.083829\n",
      "Epoch 175, loss: 23001.982665\n",
      "Epoch 176, loss: 21371.753247\n",
      "Epoch 177, loss: 22043.629708\n",
      "Epoch 178, loss: 23041.026011\n",
      "Epoch 179, loss: 21680.843455\n",
      "Epoch 180, loss: 23297.896706\n",
      "Epoch 181, loss: 21776.774168\n",
      "Epoch 182, loss: 21223.378157\n",
      "Epoch 183, loss: 23359.232244\n",
      "Epoch 184, loss: 21700.820742\n",
      "Epoch 185, loss: 23185.947764\n",
      "Epoch 186, loss: 21029.793048\n",
      "Epoch 187, loss: 21908.804289\n",
      "Epoch 188, loss: 21469.240003\n",
      "Epoch 189, loss: 23564.309717\n",
      "Epoch 190, loss: 21289.737922\n",
      "Epoch 191, loss: 22573.172456\n",
      "Epoch 192, loss: 19974.750355\n",
      "Epoch 193, loss: 22715.440577\n",
      "Epoch 194, loss: 23317.422277\n",
      "Epoch 195, loss: 22690.522596\n",
      "Epoch 196, loss: 21161.701857\n",
      "Epoch 197, loss: 22446.370454\n",
      "Epoch 198, loss: 22164.194208\n",
      "Epoch 199, loss: 23538.642389\n",
      "0.001 1e-06 0.194\n",
      "Epoch 0, loss: 20645.004856\n",
      "Epoch 1, loss: 20450.317094\n",
      "Epoch 2, loss: 20295.852344\n",
      "Epoch 3, loss: 20155.793417\n",
      "Epoch 4, loss: 20048.208574\n",
      "Epoch 5, loss: 19950.996634\n",
      "Epoch 6, loss: 19861.099088\n",
      "Epoch 7, loss: 19794.952912\n",
      "Epoch 8, loss: 19740.489225\n",
      "Epoch 9, loss: 19679.471883\n",
      "Epoch 10, loss: 19638.835217\n",
      "Epoch 11, loss: 19594.457752\n",
      "Epoch 12, loss: 19558.079324\n",
      "Epoch 13, loss: 19526.520838\n",
      "Epoch 14, loss: 19497.435868\n",
      "Epoch 15, loss: 19468.440180\n",
      "Epoch 16, loss: 19443.446215\n",
      "Epoch 17, loss: 19421.259732\n",
      "Epoch 18, loss: 19393.851926\n",
      "Epoch 19, loss: 19369.977890\n",
      "Epoch 20, loss: 19356.615458\n",
      "Epoch 21, loss: 19334.853026\n",
      "Epoch 22, loss: 19318.543697\n",
      "Epoch 23, loss: 19301.870414\n",
      "Epoch 24, loss: 19289.883759\n",
      "Epoch 25, loss: 19271.646443\n",
      "Epoch 26, loss: 19250.770903\n",
      "Epoch 27, loss: 19244.195725\n",
      "Epoch 28, loss: 19231.975842\n",
      "Epoch 29, loss: 19218.941935\n",
      "Epoch 30, loss: 19198.948621\n",
      "Epoch 31, loss: 19192.770445\n",
      "Epoch 32, loss: 19178.843515\n",
      "Epoch 33, loss: 19170.701557\n",
      "Epoch 34, loss: 19159.909155\n",
      "Epoch 35, loss: 19141.869529\n",
      "Epoch 36, loss: 19138.239068\n",
      "Epoch 37, loss: 19137.040521\n",
      "Epoch 38, loss: 19121.955119\n",
      "Epoch 39, loss: 19113.488510\n",
      "Epoch 40, loss: 19098.802788\n",
      "Epoch 41, loss: 19094.863130\n",
      "Epoch 42, loss: 19088.223942\n",
      "Epoch 43, loss: 19077.050168\n",
      "Epoch 44, loss: 19060.030360\n",
      "Epoch 45, loss: 19060.637393\n",
      "Epoch 46, loss: 19052.958062\n",
      "Epoch 47, loss: 19042.003542\n",
      "Epoch 48, loss: 19039.569024\n",
      "Epoch 49, loss: 19031.918527\n",
      "Epoch 50, loss: 19022.136880\n",
      "Epoch 51, loss: 19011.525061\n",
      "Epoch 52, loss: 19012.877182\n",
      "Epoch 53, loss: 19007.481618\n",
      "Epoch 54, loss: 18994.099396\n",
      "Epoch 55, loss: 18991.404502\n",
      "Epoch 56, loss: 18986.550207\n",
      "Epoch 57, loss: 18981.619975\n",
      "Epoch 58, loss: 18971.310543\n",
      "Epoch 59, loss: 18966.792756\n",
      "Epoch 60, loss: 18963.158226\n",
      "Epoch 61, loss: 18951.121580\n",
      "Epoch 62, loss: 18949.406105\n",
      "Epoch 63, loss: 18934.703540\n",
      "Epoch 64, loss: 18933.517572\n",
      "Epoch 65, loss: 18928.040687\n",
      "Epoch 66, loss: 18924.572120\n",
      "Epoch 67, loss: 18919.122881\n",
      "Epoch 68, loss: 18910.974690\n",
      "Epoch 69, loss: 18912.610761\n",
      "Epoch 70, loss: 18905.433501\n",
      "Epoch 71, loss: 18895.008354\n",
      "Epoch 72, loss: 18894.257975\n",
      "Epoch 73, loss: 18884.722705\n",
      "Epoch 74, loss: 18882.506871\n",
      "Epoch 75, loss: 18880.015161\n",
      "Epoch 76, loss: 18876.137784\n",
      "Epoch 77, loss: 18870.919727\n",
      "Epoch 78, loss: 18869.658321\n",
      "Epoch 79, loss: 18860.224122\n",
      "Epoch 80, loss: 18849.597443\n",
      "Epoch 81, loss: 18849.718811\n",
      "Epoch 82, loss: 18846.921496\n",
      "Epoch 83, loss: 18836.747043\n",
      "Epoch 84, loss: 18841.702183\n",
      "Epoch 85, loss: 18834.206809\n",
      "Epoch 86, loss: 18830.436870\n",
      "Epoch 87, loss: 18824.387204\n",
      "Epoch 88, loss: 18824.192837\n",
      "Epoch 89, loss: 18819.706481\n",
      "Epoch 90, loss: 18813.910260\n",
      "Epoch 91, loss: 18810.881891\n",
      "Epoch 92, loss: 18808.134421\n",
      "Epoch 93, loss: 18802.951137\n",
      "Epoch 94, loss: 18792.143728\n",
      "Epoch 95, loss: 18799.930871\n",
      "Epoch 96, loss: 18789.936370\n",
      "Epoch 97, loss: 18785.723500\n",
      "Epoch 98, loss: 18784.880140\n",
      "Epoch 99, loss: 18778.589214\n",
      "Epoch 100, loss: 18779.179356\n",
      "Epoch 101, loss: 18774.191526\n",
      "Epoch 102, loss: 18764.435091\n",
      "Epoch 103, loss: 18771.086158\n",
      "Epoch 104, loss: 18761.063789\n",
      "Epoch 105, loss: 18754.804688\n",
      "Epoch 106, loss: 18756.300043\n",
      "Epoch 107, loss: 18753.082354\n",
      "Epoch 108, loss: 18745.319394\n",
      "Epoch 109, loss: 18745.652825\n",
      "Epoch 110, loss: 18741.058597\n",
      "Epoch 111, loss: 18741.501742\n",
      "Epoch 112, loss: 18734.174843\n",
      "Epoch 113, loss: 18734.247879\n",
      "Epoch 114, loss: 18733.073501\n",
      "Epoch 115, loss: 18726.873046\n",
      "Epoch 116, loss: 18724.300099\n",
      "Epoch 117, loss: 18724.248478\n",
      "Epoch 118, loss: 18712.793150\n",
      "Epoch 119, loss: 18706.345343\n",
      "Epoch 120, loss: 18709.101805\n",
      "Epoch 121, loss: 18709.344050\n",
      "Epoch 122, loss: 18703.500065\n",
      "Epoch 123, loss: 18701.143961\n",
      "Epoch 124, loss: 18695.334493\n",
      "Epoch 125, loss: 18700.232866\n",
      "Epoch 126, loss: 18689.543553\n",
      "Epoch 127, loss: 18687.898734\n",
      "Epoch 128, loss: 18686.456053\n",
      "Epoch 129, loss: 18683.574314\n",
      "Epoch 130, loss: 18680.249600\n",
      "Epoch 131, loss: 18677.569247\n",
      "Epoch 132, loss: 18677.348047\n",
      "Epoch 133, loss: 18676.893717\n",
      "Epoch 134, loss: 18675.003413\n",
      "Epoch 135, loss: 18665.985404\n",
      "Epoch 136, loss: 18662.808389\n",
      "Epoch 137, loss: 18660.659824\n",
      "Epoch 138, loss: 18658.665401\n",
      "Epoch 139, loss: 18656.800062\n",
      "Epoch 140, loss: 18650.687957\n",
      "Epoch 141, loss: 18650.040120\n",
      "Epoch 142, loss: 18647.086232\n",
      "Epoch 143, loss: 18643.283831\n",
      "Epoch 144, loss: 18645.543735\n",
      "Epoch 145, loss: 18642.114413\n",
      "Epoch 146, loss: 18637.248162\n",
      "Epoch 147, loss: 18633.643516\n",
      "Epoch 148, loss: 18632.437505\n",
      "Epoch 149, loss: 18631.849348\n",
      "Epoch 150, loss: 18630.458756\n",
      "Epoch 151, loss: 18624.360957\n",
      "Epoch 152, loss: 18628.075667\n",
      "Epoch 153, loss: 18622.450967\n",
      "Epoch 154, loss: 18618.782205\n",
      "Epoch 155, loss: 18608.730862\n",
      "Epoch 156, loss: 18619.690752\n",
      "Epoch 157, loss: 18610.542806\n",
      "Epoch 158, loss: 18611.910274\n",
      "Epoch 159, loss: 18612.386125\n",
      "Epoch 160, loss: 18604.911693\n",
      "Epoch 161, loss: 18597.896603\n",
      "Epoch 162, loss: 18598.367837\n",
      "Epoch 163, loss: 18590.995726\n",
      "Epoch 164, loss: 18594.815954\n",
      "Epoch 165, loss: 18587.862318\n",
      "Epoch 166, loss: 18596.920242\n",
      "Epoch 167, loss: 18587.882019\n",
      "Epoch 168, loss: 18590.331163\n",
      "Epoch 169, loss: 18581.107040\n",
      "Epoch 170, loss: 18579.576585\n",
      "Epoch 171, loss: 18579.927150\n",
      "Epoch 172, loss: 18578.672491\n",
      "Epoch 173, loss: 18577.969478\n",
      "Epoch 174, loss: 18572.936555\n",
      "Epoch 175, loss: 18566.369696\n",
      "Epoch 176, loss: 18568.298668\n",
      "Epoch 177, loss: 18565.955884\n",
      "Epoch 178, loss: 18563.284628\n",
      "Epoch 179, loss: 18563.253586\n",
      "Epoch 180, loss: 18562.093866\n",
      "Epoch 181, loss: 18556.496199\n",
      "Epoch 182, loss: 18549.647798\n",
      "Epoch 183, loss: 18557.996691\n",
      "Epoch 184, loss: 18548.103121\n",
      "Epoch 185, loss: 18542.434900\n",
      "Epoch 186, loss: 18548.225877\n",
      "Epoch 187, loss: 18544.369600\n",
      "Epoch 188, loss: 18538.211982\n",
      "Epoch 189, loss: 18538.451417\n",
      "Epoch 190, loss: 18538.655163\n",
      "Epoch 191, loss: 18531.684100\n",
      "Epoch 192, loss: 18533.230486\n",
      "Epoch 193, loss: 18527.070664\n",
      "Epoch 194, loss: 18529.760051\n",
      "Epoch 195, loss: 18524.513870\n",
      "Epoch 196, loss: 18526.536026\n",
      "Epoch 197, loss: 18518.146085\n",
      "Epoch 198, loss: 18518.276596\n",
      "Epoch 199, loss: 18518.336962\n",
      "0.0001 0.0001 0.247\n",
      "Epoch 0, loss: 20650.416608\n",
      "Epoch 1, loss: 20448.145943\n",
      "Epoch 2, loss: 20287.750035\n",
      "Epoch 3, loss: 20151.277240\n",
      "Epoch 4, loss: 20045.144571\n",
      "Epoch 5, loss: 19953.352107\n",
      "Epoch 6, loss: 19864.835106\n",
      "Epoch 7, loss: 19796.798439\n",
      "Epoch 8, loss: 19736.513695\n",
      "Epoch 9, loss: 19681.322967\n",
      "Epoch 10, loss: 19636.364201\n",
      "Epoch 11, loss: 19591.771327\n",
      "Epoch 12, loss: 19556.301053\n",
      "Epoch 13, loss: 19523.377181\n",
      "Epoch 14, loss: 19490.129548\n",
      "Epoch 15, loss: 19469.686238\n",
      "Epoch 16, loss: 19440.618697\n",
      "Epoch 17, loss: 19414.885759\n",
      "Epoch 18, loss: 19398.508006\n",
      "Epoch 19, loss: 19371.013108\n",
      "Epoch 20, loss: 19351.989838\n",
      "Epoch 21, loss: 19338.058270\n",
      "Epoch 22, loss: 19314.783373\n",
      "Epoch 23, loss: 19300.193995\n",
      "Epoch 24, loss: 19286.862990\n",
      "Epoch 25, loss: 19270.526907\n",
      "Epoch 26, loss: 19255.918611\n",
      "Epoch 27, loss: 19240.856368\n",
      "Epoch 28, loss: 19225.050725\n",
      "Epoch 29, loss: 19219.272408\n",
      "Epoch 30, loss: 19200.177515\n",
      "Epoch 31, loss: 19193.318050\n",
      "Epoch 32, loss: 19178.765094\n",
      "Epoch 33, loss: 19170.898952\n",
      "Epoch 34, loss: 19157.832216\n",
      "Epoch 35, loss: 19145.973308\n",
      "Epoch 36, loss: 19137.342690\n",
      "Epoch 37, loss: 19131.520695\n",
      "Epoch 38, loss: 19118.560758\n",
      "Epoch 39, loss: 19104.337862\n",
      "Epoch 40, loss: 19101.840275\n",
      "Epoch 41, loss: 19089.283360\n",
      "Epoch 42, loss: 19081.642290\n",
      "Epoch 43, loss: 19079.270659\n",
      "Epoch 44, loss: 19060.913334\n",
      "Epoch 45, loss: 19047.501851\n",
      "Epoch 46, loss: 19053.614548\n",
      "Epoch 47, loss: 19043.142942\n",
      "Epoch 48, loss: 19036.536610\n",
      "Epoch 49, loss: 19025.882709\n",
      "Epoch 50, loss: 19021.693876\n",
      "Epoch 51, loss: 19014.625430\n",
      "Epoch 52, loss: 19001.563310\n",
      "Epoch 53, loss: 19000.525963\n",
      "Epoch 54, loss: 18989.520030\n",
      "Epoch 55, loss: 18990.485188\n",
      "Epoch 56, loss: 18978.214053\n",
      "Epoch 57, loss: 18978.603949\n",
      "Epoch 58, loss: 18966.232996\n",
      "Epoch 59, loss: 18967.095228\n",
      "Epoch 60, loss: 18956.064283\n",
      "Epoch 61, loss: 18953.660502\n",
      "Epoch 62, loss: 18950.385949\n",
      "Epoch 63, loss: 18940.497491\n",
      "Epoch 64, loss: 18935.127625\n",
      "Epoch 65, loss: 18925.146242\n",
      "Epoch 66, loss: 18923.077658\n",
      "Epoch 67, loss: 18919.045811\n",
      "Epoch 68, loss: 18913.799487\n",
      "Epoch 69, loss: 18907.537192\n",
      "Epoch 70, loss: 18902.702531\n",
      "Epoch 71, loss: 18894.833300\n",
      "Epoch 72, loss: 18894.047167\n",
      "Epoch 73, loss: 18888.830598\n",
      "Epoch 74, loss: 18885.611533\n",
      "Epoch 75, loss: 18877.557469\n",
      "Epoch 76, loss: 18875.368754\n",
      "Epoch 77, loss: 18867.537998\n",
      "Epoch 78, loss: 18858.050539\n",
      "Epoch 79, loss: 18859.209344\n",
      "Epoch 80, loss: 18855.678912\n",
      "Epoch 81, loss: 18850.059779\n",
      "Epoch 82, loss: 18846.917783\n",
      "Epoch 83, loss: 18844.997875\n",
      "Epoch 84, loss: 18836.588162\n",
      "Epoch 85, loss: 18831.812527\n",
      "Epoch 86, loss: 18829.728592\n",
      "Epoch 87, loss: 18829.747027\n",
      "Epoch 88, loss: 18823.008665\n",
      "Epoch 89, loss: 18820.145970\n",
      "Epoch 90, loss: 18815.267657\n",
      "Epoch 91, loss: 18814.702559\n",
      "Epoch 92, loss: 18806.438726\n",
      "Epoch 93, loss: 18806.157781\n",
      "Epoch 94, loss: 18793.287689\n",
      "Epoch 95, loss: 18793.525659\n",
      "Epoch 96, loss: 18794.002026\n",
      "Epoch 97, loss: 18787.935227\n",
      "Epoch 98, loss: 18782.511942\n",
      "Epoch 99, loss: 18778.965191\n",
      "Epoch 100, loss: 18778.493742\n",
      "Epoch 101, loss: 18774.719545\n",
      "Epoch 102, loss: 18769.419140\n",
      "Epoch 103, loss: 18764.013444\n",
      "Epoch 104, loss: 18764.154322\n",
      "Epoch 105, loss: 18756.279688\n",
      "Epoch 106, loss: 18755.416544\n",
      "Epoch 107, loss: 18749.095238\n",
      "Epoch 108, loss: 18747.695023\n",
      "Epoch 109, loss: 18750.934746\n",
      "Epoch 110, loss: 18740.066644\n",
      "Epoch 111, loss: 18742.461740\n",
      "Epoch 112, loss: 18731.512601\n",
      "Epoch 113, loss: 18730.107132\n",
      "Epoch 114, loss: 18733.728179\n",
      "Epoch 115, loss: 18727.797618\n",
      "Epoch 116, loss: 18719.957710\n",
      "Epoch 117, loss: 18709.704336\n",
      "Epoch 118, loss: 18716.240406\n",
      "Epoch 119, loss: 18716.241010\n",
      "Epoch 120, loss: 18707.708890\n",
      "Epoch 121, loss: 18712.279676\n",
      "Epoch 122, loss: 18701.405593\n",
      "Epoch 123, loss: 18696.963572\n",
      "Epoch 124, loss: 18699.062898\n",
      "Epoch 125, loss: 18699.324587\n",
      "Epoch 126, loss: 18691.323756\n",
      "Epoch 127, loss: 18690.516864\n",
      "Epoch 128, loss: 18687.741760\n",
      "Epoch 129, loss: 18686.192280\n",
      "Epoch 130, loss: 18682.666519\n",
      "Epoch 131, loss: 18675.500848\n",
      "Epoch 132, loss: 18677.057464\n",
      "Epoch 133, loss: 18674.832905\n",
      "Epoch 134, loss: 18672.105478\n",
      "Epoch 135, loss: 18671.550294\n",
      "Epoch 136, loss: 18668.610121\n",
      "Epoch 137, loss: 18667.422090\n",
      "Epoch 138, loss: 18654.881251\n",
      "Epoch 139, loss: 18658.265997\n",
      "Epoch 140, loss: 18655.558219\n",
      "Epoch 141, loss: 18647.732406\n",
      "Epoch 142, loss: 18648.086590\n",
      "Epoch 143, loss: 18648.718764\n",
      "Epoch 144, loss: 18642.686829\n",
      "Epoch 145, loss: 18638.509980\n",
      "Epoch 146, loss: 18633.826382\n",
      "Epoch 147, loss: 18634.107692\n",
      "Epoch 148, loss: 18629.738248\n",
      "Epoch 149, loss: 18629.681882\n",
      "Epoch 150, loss: 18625.754106\n",
      "Epoch 151, loss: 18627.661937\n",
      "Epoch 152, loss: 18626.223954\n",
      "Epoch 153, loss: 18612.680014\n",
      "Epoch 154, loss: 18611.155777\n",
      "Epoch 155, loss: 18622.333930\n",
      "Epoch 156, loss: 18616.768895\n",
      "Epoch 157, loss: 18609.641708\n",
      "Epoch 158, loss: 18612.066244\n",
      "Epoch 159, loss: 18604.673248\n",
      "Epoch 160, loss: 18606.398303\n",
      "Epoch 161, loss: 18602.075052\n",
      "Epoch 162, loss: 18593.459001\n",
      "Epoch 163, loss: 18596.798909\n",
      "Epoch 164, loss: 18590.315178\n",
      "Epoch 165, loss: 18589.560712\n",
      "Epoch 166, loss: 18588.194665\n",
      "Epoch 167, loss: 18586.645493\n",
      "Epoch 168, loss: 18580.831026\n",
      "Epoch 169, loss: 18583.576785\n",
      "Epoch 170, loss: 18578.103455\n",
      "Epoch 171, loss: 18580.714815\n",
      "Epoch 172, loss: 18581.387936\n",
      "Epoch 173, loss: 18573.760786\n",
      "Epoch 174, loss: 18566.655026\n",
      "Epoch 175, loss: 18565.600039\n",
      "Epoch 176, loss: 18561.992941\n",
      "Epoch 177, loss: 18561.158589\n",
      "Epoch 178, loss: 18560.185932\n",
      "Epoch 179, loss: 18555.221982\n",
      "Epoch 180, loss: 18561.942566\n",
      "Epoch 181, loss: 18555.259385\n",
      "Epoch 182, loss: 18555.541193\n",
      "Epoch 183, loss: 18555.399481\n",
      "Epoch 184, loss: 18553.054642\n",
      "Epoch 185, loss: 18553.636166\n",
      "Epoch 186, loss: 18542.543058\n",
      "Epoch 187, loss: 18547.000249\n",
      "Epoch 188, loss: 18546.662088\n",
      "Epoch 189, loss: 18540.452040\n",
      "Epoch 190, loss: 18536.589920\n",
      "Epoch 191, loss: 18535.777461\n",
      "Epoch 192, loss: 18535.659418\n",
      "Epoch 193, loss: 18531.854585\n",
      "Epoch 194, loss: 18531.701427\n",
      "Epoch 195, loss: 18523.233179\n",
      "Epoch 196, loss: 18523.325928\n",
      "Epoch 197, loss: 18520.945875\n",
      "Epoch 198, loss: 18520.087046\n",
      "Epoch 199, loss: 18520.414662\n",
      "0.0001 1e-05 0.244\n",
      "Epoch 0, loss: 20645.107777\n",
      "Epoch 1, loss: 20445.152632\n",
      "Epoch 2, loss: 20294.288560\n",
      "Epoch 3, loss: 20156.622829\n",
      "Epoch 4, loss: 20042.501589\n",
      "Epoch 5, loss: 19950.576271\n",
      "Epoch 6, loss: 19866.479382\n",
      "Epoch 7, loss: 19797.535738\n",
      "Epoch 8, loss: 19738.237281\n",
      "Epoch 9, loss: 19678.152514\n",
      "Epoch 10, loss: 19640.784351\n",
      "Epoch 11, loss: 19595.284615\n",
      "Epoch 12, loss: 19560.177257\n",
      "Epoch 13, loss: 19519.674682\n",
      "Epoch 14, loss: 19496.269473\n",
      "Epoch 15, loss: 19460.613882\n",
      "Epoch 16, loss: 19442.804360\n",
      "Epoch 17, loss: 19417.239609\n",
      "Epoch 18, loss: 19392.597486\n",
      "Epoch 19, loss: 19368.311714\n",
      "Epoch 20, loss: 19353.687696\n",
      "Epoch 21, loss: 19334.251944\n",
      "Epoch 22, loss: 19315.173590\n",
      "Epoch 23, loss: 19304.366813\n",
      "Epoch 24, loss: 19281.988838\n",
      "Epoch 25, loss: 19271.344600\n",
      "Epoch 26, loss: 19252.650976\n",
      "Epoch 27, loss: 19244.497996\n",
      "Epoch 28, loss: 19230.093314\n",
      "Epoch 29, loss: 19217.765304\n",
      "Epoch 30, loss: 19208.383873\n",
      "Epoch 31, loss: 19190.050926\n",
      "Epoch 32, loss: 19180.915538\n",
      "Epoch 33, loss: 19172.326723\n",
      "Epoch 34, loss: 19158.021911\n",
      "Epoch 35, loss: 19148.651253\n",
      "Epoch 36, loss: 19136.514881\n",
      "Epoch 37, loss: 19131.985400\n",
      "Epoch 38, loss: 19121.246909\n",
      "Epoch 39, loss: 19108.514149\n",
      "Epoch 40, loss: 19098.546786\n",
      "Epoch 41, loss: 19092.198305\n",
      "Epoch 42, loss: 19078.895786\n",
      "Epoch 43, loss: 19072.850821\n",
      "Epoch 44, loss: 19069.407674\n",
      "Epoch 45, loss: 19054.991465\n",
      "Epoch 46, loss: 19052.468180\n",
      "Epoch 47, loss: 19051.618858\n",
      "Epoch 48, loss: 19036.713115\n",
      "Epoch 49, loss: 19025.998046\n",
      "Epoch 50, loss: 19014.032951\n",
      "Epoch 51, loss: 19014.423773\n",
      "Epoch 52, loss: 19007.202981\n",
      "Epoch 53, loss: 19007.004115\n",
      "Epoch 54, loss: 18989.757301\n",
      "Epoch 55, loss: 18990.029673\n",
      "Epoch 56, loss: 18978.317497\n",
      "Epoch 57, loss: 18979.713900\n",
      "Epoch 58, loss: 18962.658243\n",
      "Epoch 59, loss: 18965.911279\n",
      "Epoch 60, loss: 18957.839777\n",
      "Epoch 61, loss: 18955.519494\n",
      "Epoch 62, loss: 18945.318857\n",
      "Epoch 63, loss: 18941.318915\n",
      "Epoch 64, loss: 18926.495807\n",
      "Epoch 65, loss: 18933.610770\n",
      "Epoch 66, loss: 18926.285367\n",
      "Epoch 67, loss: 18913.760514\n",
      "Epoch 68, loss: 18912.614556\n",
      "Epoch 69, loss: 18905.305720\n",
      "Epoch 70, loss: 18907.661276\n",
      "Epoch 71, loss: 18893.658590\n",
      "Epoch 72, loss: 18891.193630\n",
      "Epoch 73, loss: 18890.734041\n",
      "Epoch 74, loss: 18881.178686\n",
      "Epoch 75, loss: 18875.109846\n",
      "Epoch 76, loss: 18870.769868\n",
      "Epoch 77, loss: 18864.466745\n",
      "Epoch 78, loss: 18865.215258\n",
      "Epoch 79, loss: 18860.205280\n",
      "Epoch 80, loss: 18860.023338\n",
      "Epoch 81, loss: 18854.609425\n",
      "Epoch 82, loss: 18848.262890\n",
      "Epoch 83, loss: 18841.875239\n",
      "Epoch 84, loss: 18843.522084\n",
      "Epoch 85, loss: 18835.615578\n",
      "Epoch 86, loss: 18826.438794\n",
      "Epoch 87, loss: 18828.553378\n",
      "Epoch 88, loss: 18818.454177\n",
      "Epoch 89, loss: 18816.403621\n",
      "Epoch 90, loss: 18820.237512\n",
      "Epoch 91, loss: 18810.467183\n",
      "Epoch 92, loss: 18810.523417\n",
      "Epoch 93, loss: 18801.029763\n",
      "Epoch 94, loss: 18803.118311\n",
      "Epoch 95, loss: 18793.896828\n",
      "Epoch 96, loss: 18795.109693\n",
      "Epoch 97, loss: 18792.381507\n",
      "Epoch 98, loss: 18783.991084\n",
      "Epoch 99, loss: 18778.171142\n",
      "Epoch 100, loss: 18775.031346\n",
      "Epoch 101, loss: 18771.066907\n",
      "Epoch 102, loss: 18771.685104\n",
      "Epoch 103, loss: 18767.867444\n",
      "Epoch 104, loss: 18769.504813\n",
      "Epoch 105, loss: 18762.265068\n",
      "Epoch 106, loss: 18754.580747\n",
      "Epoch 107, loss: 18755.363688\n",
      "Epoch 108, loss: 18749.800444\n",
      "Epoch 109, loss: 18746.360720\n",
      "Epoch 110, loss: 18742.235799\n",
      "Epoch 111, loss: 18736.122411\n",
      "Epoch 112, loss: 18741.025523\n",
      "Epoch 113, loss: 18735.333335\n",
      "Epoch 114, loss: 18728.466331\n",
      "Epoch 115, loss: 18727.596613\n",
      "Epoch 116, loss: 18722.702876\n",
      "Epoch 117, loss: 18719.336569\n",
      "Epoch 118, loss: 18719.060131\n",
      "Epoch 119, loss: 18709.590425\n",
      "Epoch 120, loss: 18707.059072\n",
      "Epoch 121, loss: 18703.911200\n",
      "Epoch 122, loss: 18706.042808\n",
      "Epoch 123, loss: 18696.082882\n",
      "Epoch 124, loss: 18697.197155\n",
      "Epoch 125, loss: 18695.055933\n",
      "Epoch 126, loss: 18692.919510\n",
      "Epoch 127, loss: 18682.749643\n",
      "Epoch 128, loss: 18688.106962\n",
      "Epoch 129, loss: 18686.075456\n",
      "Epoch 130, loss: 18674.707844\n",
      "Epoch 131, loss: 18680.766043\n",
      "Epoch 132, loss: 18674.458585\n",
      "Epoch 133, loss: 18672.767326\n",
      "Epoch 134, loss: 18675.325542\n",
      "Epoch 135, loss: 18663.778297\n",
      "Epoch 136, loss: 18660.621702\n",
      "Epoch 137, loss: 18665.340995\n",
      "Epoch 138, loss: 18656.758504\n",
      "Epoch 139, loss: 18652.691895\n",
      "Epoch 140, loss: 18652.116330\n",
      "Epoch 141, loss: 18652.870424\n",
      "Epoch 142, loss: 18648.602884\n",
      "Epoch 143, loss: 18646.348149\n",
      "Epoch 144, loss: 18639.737159\n",
      "Epoch 145, loss: 18640.491606\n",
      "Epoch 146, loss: 18633.701457\n",
      "Epoch 147, loss: 18635.315437\n",
      "Epoch 148, loss: 18636.367114\n",
      "Epoch 149, loss: 18633.931827\n",
      "Epoch 150, loss: 18626.100445\n",
      "Epoch 151, loss: 18627.781529\n",
      "Epoch 152, loss: 18622.467665\n",
      "Epoch 153, loss: 18616.685362\n",
      "Epoch 154, loss: 18620.035284\n",
      "Epoch 155, loss: 18617.731963\n",
      "Epoch 156, loss: 18610.157358\n",
      "Epoch 157, loss: 18608.973322\n",
      "Epoch 158, loss: 18604.227898\n",
      "Epoch 159, loss: 18601.353213\n",
      "Epoch 160, loss: 18605.889397\n",
      "Epoch 161, loss: 18600.664306\n",
      "Epoch 162, loss: 18597.500218\n",
      "Epoch 163, loss: 18595.612538\n",
      "Epoch 164, loss: 18595.778713\n",
      "Epoch 165, loss: 18592.159375\n",
      "Epoch 166, loss: 18584.669613\n",
      "Epoch 167, loss: 18593.342845\n",
      "Epoch 168, loss: 18582.183971\n",
      "Epoch 169, loss: 18577.448858\n",
      "Epoch 170, loss: 18581.658038\n",
      "Epoch 171, loss: 18575.686215\n",
      "Epoch 172, loss: 18569.361005\n",
      "Epoch 173, loss: 18575.972571\n",
      "Epoch 174, loss: 18574.195910\n",
      "Epoch 175, loss: 18567.193394\n",
      "Epoch 176, loss: 18564.438557\n",
      "Epoch 177, loss: 18563.925272\n",
      "Epoch 178, loss: 18560.849390\n",
      "Epoch 179, loss: 18557.146004\n",
      "Epoch 180, loss: 18554.620001\n",
      "Epoch 181, loss: 18558.942538\n",
      "Epoch 182, loss: 18552.063354\n",
      "Epoch 183, loss: 18547.446203\n",
      "Epoch 184, loss: 18545.289210\n",
      "Epoch 185, loss: 18550.564487\n",
      "Epoch 186, loss: 18545.800494\n",
      "Epoch 187, loss: 18542.574276\n",
      "Epoch 188, loss: 18538.591555\n",
      "Epoch 189, loss: 18541.994224\n",
      "Epoch 190, loss: 18533.679423\n",
      "Epoch 191, loss: 18538.972183\n",
      "Epoch 192, loss: 18533.732701\n",
      "Epoch 193, loss: 18533.941235\n",
      "Epoch 194, loss: 18525.560487\n",
      "Epoch 195, loss: 18521.257261\n",
      "Epoch 196, loss: 18522.524349\n",
      "Epoch 197, loss: 18520.725599\n",
      "Epoch 198, loss: 18516.660846\n",
      "Epoch 199, loss: 18522.101808\n",
      "0.0001 1e-06 0.254\n",
      "Epoch 0, loss: 20714.873841\n",
      "Epoch 1, loss: 20689.020148\n",
      "Epoch 2, loss: 20665.057664\n",
      "Epoch 3, loss: 20641.596220\n",
      "Epoch 4, loss: 20619.266578\n",
      "Epoch 5, loss: 20597.397008\n",
      "Epoch 6, loss: 20576.206215\n",
      "Epoch 7, loss: 20555.127659\n",
      "Epoch 8, loss: 20534.387274\n",
      "Epoch 9, loss: 20515.536912\n",
      "Epoch 10, loss: 20495.244532\n",
      "Epoch 11, loss: 20476.383552\n",
      "Epoch 12, loss: 20458.037458\n",
      "Epoch 13, loss: 20439.406561\n",
      "Epoch 14, loss: 20421.473677\n",
      "Epoch 15, loss: 20404.832584\n",
      "Epoch 16, loss: 20386.703944\n",
      "Epoch 17, loss: 20370.565075\n",
      "Epoch 18, loss: 20353.451777\n",
      "Epoch 19, loss: 20337.848377\n",
      "Epoch 20, loss: 20321.425953\n",
      "Epoch 21, loss: 20305.517451\n",
      "Epoch 22, loss: 20290.806791\n",
      "Epoch 23, loss: 20275.351545\n",
      "Epoch 24, loss: 20260.182957\n",
      "Epoch 25, loss: 20246.009149\n",
      "Epoch 26, loss: 20232.141110\n",
      "Epoch 27, loss: 20217.834529\n",
      "Epoch 28, loss: 20204.617693\n",
      "Epoch 29, loss: 20190.903941\n",
      "Epoch 30, loss: 20177.701724\n",
      "Epoch 31, loss: 20164.406781\n",
      "Epoch 32, loss: 20152.430403\n",
      "Epoch 33, loss: 20139.251023\n",
      "Epoch 34, loss: 20127.012848\n",
      "Epoch 35, loss: 20114.694230\n",
      "Epoch 36, loss: 20102.640071\n",
      "Epoch 37, loss: 20091.596694\n",
      "Epoch 38, loss: 20079.220442\n",
      "Epoch 39, loss: 20068.040549\n",
      "Epoch 40, loss: 20056.974972\n",
      "Epoch 41, loss: 20045.744660\n",
      "Epoch 42, loss: 20035.253625\n",
      "Epoch 43, loss: 20023.941792\n",
      "Epoch 44, loss: 20014.042749\n",
      "Epoch 45, loss: 20004.148617\n",
      "Epoch 46, loss: 19993.947175\n",
      "Epoch 47, loss: 19983.828324\n",
      "Epoch 48, loss: 19973.618418\n",
      "Epoch 49, loss: 19964.422369\n",
      "Epoch 50, loss: 19954.790747\n",
      "Epoch 51, loss: 19945.402343\n",
      "Epoch 52, loss: 19936.361962\n",
      "Epoch 53, loss: 19927.058304\n",
      "Epoch 54, loss: 19918.684519\n",
      "Epoch 55, loss: 19909.537507\n",
      "Epoch 56, loss: 19900.733422\n",
      "Epoch 57, loss: 19892.363859\n",
      "Epoch 58, loss: 19883.928444\n",
      "Epoch 59, loss: 19875.962311\n",
      "Epoch 60, loss: 19867.973133\n",
      "Epoch 61, loss: 19859.944571\n",
      "Epoch 62, loss: 19852.262987\n",
      "Epoch 63, loss: 19844.509441\n",
      "Epoch 64, loss: 19837.020949\n",
      "Epoch 65, loss: 19829.380193\n",
      "Epoch 66, loss: 19821.551709\n",
      "Epoch 67, loss: 19814.971764\n",
      "Epoch 68, loss: 19807.267702\n",
      "Epoch 69, loss: 19800.463147\n",
      "Epoch 70, loss: 19793.304030\n",
      "Epoch 71, loss: 19786.219624\n",
      "Epoch 72, loss: 19780.006679\n",
      "Epoch 73, loss: 19772.676801\n",
      "Epoch 74, loss: 19766.553205\n",
      "Epoch 75, loss: 19759.678823\n",
      "Epoch 76, loss: 19754.195064\n",
      "Epoch 77, loss: 19747.580549\n",
      "Epoch 78, loss: 19740.928577\n",
      "Epoch 79, loss: 19735.364327\n",
      "Epoch 80, loss: 19728.629125\n",
      "Epoch 81, loss: 19723.106834\n",
      "Epoch 82, loss: 19717.045149\n",
      "Epoch 83, loss: 19710.914292\n",
      "Epoch 84, loss: 19705.700854\n",
      "Epoch 85, loss: 19699.762593\n",
      "Epoch 86, loss: 19694.136516\n",
      "Epoch 87, loss: 19689.267400\n",
      "Epoch 88, loss: 19683.815772\n",
      "Epoch 89, loss: 19677.636531\n",
      "Epoch 90, loss: 19672.815515\n",
      "Epoch 91, loss: 19667.313817\n",
      "Epoch 92, loss: 19663.029739\n",
      "Epoch 93, loss: 19657.837917\n",
      "Epoch 94, loss: 19652.822646\n",
      "Epoch 95, loss: 19647.586923\n",
      "Epoch 96, loss: 19642.914989\n",
      "Epoch 97, loss: 19637.752031\n",
      "Epoch 98, loss: 19633.187865\n",
      "Epoch 99, loss: 19628.433757\n",
      "Epoch 100, loss: 19624.212812\n",
      "Epoch 101, loss: 19618.963714\n",
      "Epoch 102, loss: 19614.783320\n",
      "Epoch 103, loss: 19610.422214\n",
      "Epoch 104, loss: 19605.728706\n",
      "Epoch 105, loss: 19601.228072\n",
      "Epoch 106, loss: 19596.932536\n",
      "Epoch 107, loss: 19592.540635\n",
      "Epoch 108, loss: 19588.581121\n",
      "Epoch 109, loss: 19584.424707\n",
      "Epoch 110, loss: 19580.276751\n",
      "Epoch 111, loss: 19576.633256\n",
      "Epoch 112, loss: 19571.919578\n",
      "Epoch 113, loss: 19567.967881\n",
      "Epoch 114, loss: 19564.324022\n",
      "Epoch 115, loss: 19560.453736\n",
      "Epoch 116, loss: 19556.722233\n",
      "Epoch 117, loss: 19552.187191\n",
      "Epoch 118, loss: 19548.985657\n",
      "Epoch 119, loss: 19545.063712\n",
      "Epoch 120, loss: 19541.432782\n",
      "Epoch 121, loss: 19537.775844\n",
      "Epoch 122, loss: 19534.128732\n",
      "Epoch 123, loss: 19530.260835\n",
      "Epoch 124, loss: 19526.824321\n",
      "Epoch 125, loss: 19523.051979\n",
      "Epoch 126, loss: 19520.171047\n",
      "Epoch 127, loss: 19516.343922\n",
      "Epoch 128, loss: 19513.091851\n",
      "Epoch 129, loss: 19510.062623\n",
      "Epoch 130, loss: 19506.573181\n",
      "Epoch 131, loss: 19503.149344\n",
      "Epoch 132, loss: 19499.465985\n",
      "Epoch 133, loss: 19496.783675\n",
      "Epoch 134, loss: 19493.189156\n",
      "Epoch 135, loss: 19489.769144\n",
      "Epoch 136, loss: 19487.386560\n",
      "Epoch 137, loss: 19483.797703\n",
      "Epoch 138, loss: 19480.540192\n",
      "Epoch 139, loss: 19477.455342\n",
      "Epoch 140, loss: 19474.219598\n",
      "Epoch 141, loss: 19471.955577\n",
      "Epoch 142, loss: 19468.508301\n",
      "Epoch 143, loss: 19466.109755\n",
      "Epoch 144, loss: 19462.891202\n",
      "Epoch 145, loss: 19460.100274\n",
      "Epoch 146, loss: 19457.031582\n",
      "Epoch 147, loss: 19454.071522\n",
      "Epoch 148, loss: 19451.282021\n",
      "Epoch 149, loss: 19448.155586\n",
      "Epoch 150, loss: 19445.568563\n",
      "Epoch 151, loss: 19443.053594\n",
      "Epoch 152, loss: 19439.715194\n",
      "Epoch 153, loss: 19437.345835\n",
      "Epoch 154, loss: 19434.690592\n",
      "Epoch 155, loss: 19432.138954\n",
      "Epoch 156, loss: 19429.423943\n",
      "Epoch 157, loss: 19427.089759\n",
      "Epoch 158, loss: 19423.967488\n",
      "Epoch 159, loss: 19421.841382\n",
      "Epoch 160, loss: 19419.126826\n",
      "Epoch 161, loss: 19416.959009\n",
      "Epoch 162, loss: 19414.165328\n",
      "Epoch 163, loss: 19411.032404\n",
      "Epoch 164, loss: 19408.866619\n",
      "Epoch 165, loss: 19406.739714\n",
      "Epoch 166, loss: 19403.850698\n",
      "Epoch 167, loss: 19401.345685\n",
      "Epoch 168, loss: 19399.013453\n",
      "Epoch 169, loss: 19397.235234\n",
      "Epoch 170, loss: 19394.336694\n",
      "Epoch 171, loss: 19392.515901\n",
      "Epoch 172, loss: 19389.487861\n",
      "Epoch 173, loss: 19387.671351\n",
      "Epoch 174, loss: 19385.224066\n",
      "Epoch 175, loss: 19382.482948\n",
      "Epoch 176, loss: 19380.501767\n",
      "Epoch 177, loss: 19378.418425\n",
      "Epoch 178, loss: 19376.158556\n",
      "Epoch 179, loss: 19373.598404\n",
      "Epoch 180, loss: 19371.944774\n",
      "Epoch 181, loss: 19369.618708\n",
      "Epoch 182, loss: 19366.595288\n",
      "Epoch 183, loss: 19365.153952\n",
      "Epoch 184, loss: 19362.813354\n",
      "Epoch 185, loss: 19360.228290\n",
      "Epoch 186, loss: 19358.566618\n",
      "Epoch 187, loss: 19356.347915\n",
      "Epoch 188, loss: 19354.396171\n",
      "Epoch 189, loss: 19351.877720\n",
      "Epoch 190, loss: 19350.393411\n",
      "Epoch 191, loss: 19348.018223\n",
      "Epoch 192, loss: 19346.087247\n",
      "Epoch 193, loss: 19343.970942\n",
      "Epoch 194, loss: 19341.915445\n",
      "Epoch 195, loss: 19340.084258\n",
      "Epoch 196, loss: 19338.065561\n",
      "Epoch 197, loss: 19335.906796\n",
      "Epoch 198, loss: 19333.977538\n",
      "Epoch 199, loss: 19331.859299\n",
      "1e-05 0.0001 0.231\n",
      "Epoch 0, loss: 20714.435938\n",
      "Epoch 1, loss: 20688.456211\n",
      "Epoch 2, loss: 20664.011852\n",
      "Epoch 3, loss: 20640.910620\n",
      "Epoch 4, loss: 20618.412587\n",
      "Epoch 5, loss: 20597.096391\n",
      "Epoch 6, loss: 20575.023958\n",
      "Epoch 7, loss: 20554.934627\n",
      "Epoch 8, loss: 20533.985073\n",
      "Epoch 9, loss: 20514.517249\n",
      "Epoch 10, loss: 20494.722583\n",
      "Epoch 11, loss: 20475.808699\n",
      "Epoch 12, loss: 20456.834494\n",
      "Epoch 13, loss: 20438.615456\n",
      "Epoch 14, loss: 20420.519398\n",
      "Epoch 15, loss: 20403.267497\n",
      "Epoch 16, loss: 20386.330767\n",
      "Epoch 17, loss: 20369.642620\n",
      "Epoch 18, loss: 20353.060763\n",
      "Epoch 19, loss: 20336.260840\n",
      "Epoch 20, loss: 20320.648682\n",
      "Epoch 21, loss: 20305.190231\n",
      "Epoch 22, loss: 20289.719387\n",
      "Epoch 23, loss: 20274.444956\n",
      "Epoch 24, loss: 20260.051997\n",
      "Epoch 25, loss: 20245.628375\n",
      "Epoch 26, loss: 20231.275248\n",
      "Epoch 27, loss: 20217.071367\n",
      "Epoch 28, loss: 20203.408905\n",
      "Epoch 29, loss: 20189.999923\n",
      "Epoch 30, loss: 20176.690674\n",
      "Epoch 31, loss: 20163.851038\n",
      "Epoch 32, loss: 20151.185909\n",
      "Epoch 33, loss: 20138.318228\n",
      "Epoch 34, loss: 20126.075556\n",
      "Epoch 35, loss: 20113.510212\n",
      "Epoch 36, loss: 20101.559641\n",
      "Epoch 37, loss: 20090.033061\n",
      "Epoch 38, loss: 20078.708985\n",
      "Epoch 39, loss: 20067.099696\n",
      "Epoch 40, loss: 20055.966652\n",
      "Epoch 41, loss: 20044.891990\n",
      "Epoch 42, loss: 20034.338524\n",
      "Epoch 43, loss: 20023.475492\n",
      "Epoch 44, loss: 20013.202251\n",
      "Epoch 45, loss: 20002.999179\n",
      "Epoch 46, loss: 19992.441749\n",
      "Epoch 47, loss: 19983.034483\n",
      "Epoch 48, loss: 19972.499198\n",
      "Epoch 49, loss: 19963.570984\n",
      "Epoch 50, loss: 19953.575313\n",
      "Epoch 51, loss: 19944.771917\n",
      "Epoch 52, loss: 19935.587191\n",
      "Epoch 53, loss: 19926.586135\n",
      "Epoch 54, loss: 19917.239124\n",
      "Epoch 55, loss: 19908.856467\n",
      "Epoch 56, loss: 19899.957410\n",
      "Epoch 57, loss: 19891.304783\n",
      "Epoch 58, loss: 19883.553745\n",
      "Epoch 59, loss: 19875.290072\n",
      "Epoch 60, loss: 19866.839697\n",
      "Epoch 61, loss: 19858.685791\n",
      "Epoch 62, loss: 19851.619627\n",
      "Epoch 63, loss: 19843.287683\n",
      "Epoch 64, loss: 19835.732035\n",
      "Epoch 65, loss: 19827.988026\n",
      "Epoch 66, loss: 19820.780679\n",
      "Epoch 67, loss: 19813.598643\n",
      "Epoch 68, loss: 19805.936719\n",
      "Epoch 69, loss: 19799.260926\n",
      "Epoch 70, loss: 19792.406617\n",
      "Epoch 71, loss: 19785.243489\n",
      "Epoch 72, loss: 19778.358405\n",
      "Epoch 73, loss: 19771.871427\n",
      "Epoch 74, loss: 19765.544073\n",
      "Epoch 75, loss: 19759.089856\n",
      "Epoch 76, loss: 19753.205724\n",
      "Epoch 77, loss: 19746.348934\n",
      "Epoch 78, loss: 19740.439781\n",
      "Epoch 79, loss: 19734.576714\n",
      "Epoch 80, loss: 19727.965314\n",
      "Epoch 81, loss: 19721.669639\n",
      "Epoch 82, loss: 19715.775581\n",
      "Epoch 83, loss: 19710.178297\n",
      "Epoch 84, loss: 19704.732694\n",
      "Epoch 85, loss: 19699.065685\n",
      "Epoch 86, loss: 19693.027143\n",
      "Epoch 87, loss: 19687.388593\n",
      "Epoch 88, loss: 19682.777006\n",
      "Epoch 89, loss: 19676.911819\n",
      "Epoch 90, loss: 19671.589609\n",
      "Epoch 91, loss: 19666.936506\n",
      "Epoch 92, loss: 19661.742760\n",
      "Epoch 93, loss: 19656.291003\n",
      "Epoch 94, loss: 19651.216622\n",
      "Epoch 95, loss: 19646.161461\n",
      "Epoch 96, loss: 19641.469956\n",
      "Epoch 97, loss: 19636.528117\n",
      "Epoch 98, loss: 19631.852242\n",
      "Epoch 99, loss: 19627.452280\n",
      "Epoch 100, loss: 19622.959845\n",
      "Epoch 101, loss: 19617.615895\n",
      "Epoch 102, loss: 19613.497788\n",
      "Epoch 103, loss: 19609.124835\n",
      "Epoch 104, loss: 19604.429321\n",
      "Epoch 105, loss: 19600.344717\n",
      "Epoch 106, loss: 19596.097483\n",
      "Epoch 107, loss: 19592.062713\n",
      "Epoch 108, loss: 19587.535755\n",
      "Epoch 109, loss: 19583.092965\n",
      "Epoch 110, loss: 19579.380399\n",
      "Epoch 111, loss: 19575.254081\n",
      "Epoch 112, loss: 19570.487803\n",
      "Epoch 113, loss: 19566.861554\n",
      "Epoch 114, loss: 19563.257536\n",
      "Epoch 115, loss: 19559.269412\n",
      "Epoch 116, loss: 19555.404856\n",
      "Epoch 117, loss: 19550.999235\n",
      "Epoch 118, loss: 19547.424554\n",
      "Epoch 119, loss: 19544.235909\n",
      "Epoch 120, loss: 19539.994288\n",
      "Epoch 121, loss: 19536.382119\n",
      "Epoch 122, loss: 19533.062025\n",
      "Epoch 123, loss: 19529.052069\n",
      "Epoch 124, loss: 19526.047736\n",
      "Epoch 125, loss: 19522.030138\n",
      "Epoch 126, loss: 19519.045580\n",
      "Epoch 127, loss: 19515.530977\n",
      "Epoch 128, loss: 19512.104380\n",
      "Epoch 129, loss: 19508.416423\n",
      "Epoch 130, loss: 19505.048162\n",
      "Epoch 131, loss: 19501.792178\n",
      "Epoch 132, loss: 19498.257291\n",
      "Epoch 133, loss: 19495.338025\n",
      "Epoch 134, loss: 19492.488837\n",
      "Epoch 135, loss: 19489.144396\n",
      "Epoch 136, loss: 19485.604858\n",
      "Epoch 137, loss: 19482.865991\n",
      "Epoch 138, loss: 19479.254159\n",
      "Epoch 139, loss: 19476.548712\n",
      "Epoch 140, loss: 19473.504172\n",
      "Epoch 141, loss: 19470.601429\n",
      "Epoch 142, loss: 19467.387913\n",
      "Epoch 143, loss: 19464.095080\n",
      "Epoch 144, loss: 19461.366060\n",
      "Epoch 145, loss: 19458.777225\n",
      "Epoch 146, loss: 19455.618891\n",
      "Epoch 147, loss: 19452.949783\n",
      "Epoch 148, loss: 19450.405683\n",
      "Epoch 149, loss: 19447.938509\n",
      "Epoch 150, loss: 19444.860385\n",
      "Epoch 151, loss: 19442.024763\n",
      "Epoch 152, loss: 19439.202217\n",
      "Epoch 153, loss: 19437.036400\n",
      "Epoch 154, loss: 19433.608885\n",
      "Epoch 155, loss: 19431.230538\n",
      "Epoch 156, loss: 19428.418084\n",
      "Epoch 157, loss: 19426.106283\n",
      "Epoch 158, loss: 19423.419415\n",
      "Epoch 159, loss: 19420.441687\n",
      "Epoch 160, loss: 19417.844073\n",
      "Epoch 161, loss: 19415.296675\n",
      "Epoch 162, loss: 19412.550508\n",
      "Epoch 163, loss: 19410.710994\n",
      "Epoch 164, loss: 19408.022999\n",
      "Epoch 165, loss: 19405.576467\n",
      "Epoch 166, loss: 19402.842954\n",
      "Epoch 167, loss: 19400.852509\n",
      "Epoch 168, loss: 19398.384721\n",
      "Epoch 169, loss: 19395.779389\n",
      "Epoch 170, loss: 19392.900361\n",
      "Epoch 171, loss: 19390.890125\n",
      "Epoch 172, loss: 19389.077132\n",
      "Epoch 173, loss: 19386.234394\n",
      "Epoch 174, loss: 19384.182115\n",
      "Epoch 175, loss: 19381.562279\n",
      "Epoch 176, loss: 19379.281041\n",
      "Epoch 177, loss: 19377.222755\n",
      "Epoch 178, loss: 19374.893444\n",
      "Epoch 179, loss: 19372.581773\n",
      "Epoch 180, loss: 19370.428849\n",
      "Epoch 181, loss: 19368.266429\n",
      "Epoch 182, loss: 19366.096386\n",
      "Epoch 183, loss: 19364.036346\n",
      "Epoch 184, loss: 19361.746691\n",
      "Epoch 185, loss: 19359.577738\n",
      "Epoch 186, loss: 19357.747667\n",
      "Epoch 187, loss: 19354.999182\n",
      "Epoch 188, loss: 19353.679104\n",
      "Epoch 189, loss: 19351.087354\n",
      "Epoch 190, loss: 19349.557997\n",
      "Epoch 191, loss: 19347.484645\n",
      "Epoch 192, loss: 19344.691270\n",
      "Epoch 193, loss: 19342.472500\n",
      "Epoch 194, loss: 19341.743482\n",
      "Epoch 195, loss: 19339.232501\n",
      "Epoch 196, loss: 19336.640129\n",
      "Epoch 197, loss: 19334.970480\n",
      "Epoch 198, loss: 19333.277414\n",
      "Epoch 199, loss: 19330.704488\n",
      "1e-05 1e-05 0.232\n",
      "Epoch 0, loss: 20714.289382\n",
      "Epoch 1, loss: 20688.380170\n",
      "Epoch 2, loss: 20664.260468\n",
      "Epoch 3, loss: 20641.872881\n",
      "Epoch 4, loss: 20618.930301\n",
      "Epoch 5, loss: 20597.181663\n",
      "Epoch 6, loss: 20576.050879\n",
      "Epoch 7, loss: 20554.699807\n",
      "Epoch 8, loss: 20534.667429\n",
      "Epoch 9, loss: 20514.690778\n",
      "Epoch 10, loss: 20495.552202\n",
      "Epoch 11, loss: 20476.437459\n",
      "Epoch 12, loss: 20457.819453\n",
      "Epoch 13, loss: 20439.580678\n",
      "Epoch 14, loss: 20421.963043\n",
      "Epoch 15, loss: 20404.056150\n",
      "Epoch 16, loss: 20387.474371\n",
      "Epoch 17, loss: 20369.714832\n",
      "Epoch 18, loss: 20353.154084\n",
      "Epoch 19, loss: 20337.322216\n",
      "Epoch 20, loss: 20321.627985\n",
      "Epoch 21, loss: 20305.703189\n",
      "Epoch 22, loss: 20290.742768\n",
      "Epoch 23, loss: 20275.208320\n",
      "Epoch 24, loss: 20260.443848\n",
      "Epoch 25, loss: 20246.363465\n",
      "Epoch 26, loss: 20232.072221\n",
      "Epoch 27, loss: 20217.864346\n",
      "Epoch 28, loss: 20204.117164\n",
      "Epoch 29, loss: 20190.769872\n",
      "Epoch 30, loss: 20177.487905\n",
      "Epoch 31, loss: 20164.832603\n",
      "Epoch 32, loss: 20151.903653\n",
      "Epoch 33, loss: 20139.508227\n",
      "Epoch 34, loss: 20126.706579\n",
      "Epoch 35, loss: 20114.583295\n",
      "Epoch 36, loss: 20102.432480\n",
      "Epoch 37, loss: 20090.905486\n",
      "Epoch 38, loss: 20079.295795\n",
      "Epoch 39, loss: 20067.407467\n",
      "Epoch 40, loss: 20056.507085\n",
      "Epoch 41, loss: 20045.499342\n",
      "Epoch 42, loss: 20034.936686\n",
      "Epoch 43, loss: 20023.866546\n",
      "Epoch 44, loss: 20013.729734\n",
      "Epoch 45, loss: 20003.497143\n",
      "Epoch 46, loss: 19993.299079\n",
      "Epoch 47, loss: 19983.508371\n",
      "Epoch 48, loss: 19973.561539\n",
      "Epoch 49, loss: 19963.706860\n",
      "Epoch 50, loss: 19954.359969\n",
      "Epoch 51, loss: 19944.947787\n",
      "Epoch 52, loss: 19935.573251\n",
      "Epoch 53, loss: 19926.514301\n",
      "Epoch 54, loss: 19918.045260\n",
      "Epoch 55, loss: 19909.592470\n",
      "Epoch 56, loss: 19900.668366\n",
      "Epoch 57, loss: 19892.168111\n",
      "Epoch 58, loss: 19883.995464\n",
      "Epoch 59, loss: 19875.640238\n",
      "Epoch 60, loss: 19867.765894\n",
      "Epoch 61, loss: 19859.365168\n",
      "Epoch 62, loss: 19851.627459\n",
      "Epoch 63, loss: 19844.157369\n",
      "Epoch 64, loss: 19836.178821\n",
      "Epoch 65, loss: 19828.624964\n",
      "Epoch 66, loss: 19820.932400\n",
      "Epoch 67, loss: 19814.005713\n",
      "Epoch 68, loss: 19806.747261\n",
      "Epoch 69, loss: 19799.900375\n",
      "Epoch 70, loss: 19792.794763\n",
      "Epoch 71, loss: 19786.244877\n",
      "Epoch 72, loss: 19778.849663\n",
      "Epoch 73, loss: 19772.188392\n",
      "Epoch 74, loss: 19766.194995\n",
      "Epoch 75, loss: 19759.266750\n",
      "Epoch 76, loss: 19752.938985\n",
      "Epoch 77, loss: 19747.185769\n",
      "Epoch 78, loss: 19740.398119\n",
      "Epoch 79, loss: 19733.986696\n",
      "Epoch 80, loss: 19728.764545\n",
      "Epoch 81, loss: 19722.443030\n",
      "Epoch 82, loss: 19716.388142\n",
      "Epoch 83, loss: 19710.613776\n",
      "Epoch 84, loss: 19704.929480\n",
      "Epoch 85, loss: 19699.406267\n",
      "Epoch 86, loss: 19694.266768\n",
      "Epoch 87, loss: 19688.159972\n",
      "Epoch 88, loss: 19683.205372\n",
      "Epoch 89, loss: 19677.888216\n",
      "Epoch 90, loss: 19672.314187\n",
      "Epoch 91, loss: 19666.916545\n",
      "Epoch 92, loss: 19662.025516\n",
      "Epoch 93, loss: 19657.078306\n",
      "Epoch 94, loss: 19652.335276\n",
      "Epoch 95, loss: 19646.773029\n",
      "Epoch 96, loss: 19642.146672\n",
      "Epoch 97, loss: 19637.459346\n",
      "Epoch 98, loss: 19632.569234\n",
      "Epoch 99, loss: 19627.923446\n",
      "Epoch 100, loss: 19623.229555\n",
      "Epoch 101, loss: 19618.150020\n",
      "Epoch 102, loss: 19614.177490\n",
      "Epoch 103, loss: 19609.660955\n",
      "Epoch 104, loss: 19605.109907\n",
      "Epoch 105, loss: 19600.696684\n",
      "Epoch 106, loss: 19596.812984\n",
      "Epoch 107, loss: 19592.164646\n",
      "Epoch 108, loss: 19587.832107\n",
      "Epoch 109, loss: 19583.810695\n",
      "Epoch 110, loss: 19579.418642\n",
      "Epoch 111, loss: 19575.695189\n",
      "Epoch 112, loss: 19571.548611\n",
      "Epoch 113, loss: 19567.605865\n",
      "Epoch 114, loss: 19563.899941\n",
      "Epoch 115, loss: 19559.766286\n",
      "Epoch 116, loss: 19555.658576\n",
      "Epoch 117, loss: 19552.040449\n",
      "Epoch 118, loss: 19548.350830\n",
      "Epoch 119, loss: 19544.586777\n",
      "Epoch 120, loss: 19540.466336\n",
      "Epoch 121, loss: 19536.994633\n",
      "Epoch 122, loss: 19533.172035\n",
      "Epoch 123, loss: 19529.743581\n",
      "Epoch 124, loss: 19526.128335\n",
      "Epoch 125, loss: 19522.630481\n",
      "Epoch 126, loss: 19519.464845\n",
      "Epoch 127, loss: 19515.623904\n",
      "Epoch 128, loss: 19512.111617\n",
      "Epoch 129, loss: 19509.216270\n",
      "Epoch 130, loss: 19505.233644\n",
      "Epoch 131, loss: 19502.598253\n",
      "Epoch 132, loss: 19499.143653\n",
      "Epoch 133, loss: 19495.741687\n",
      "Epoch 134, loss: 19492.369363\n",
      "Epoch 135, loss: 19489.126029\n",
      "Epoch 136, loss: 19486.759040\n",
      "Epoch 137, loss: 19482.953960\n",
      "Epoch 138, loss: 19480.146717\n",
      "Epoch 139, loss: 19477.323742\n",
      "Epoch 140, loss: 19473.644573\n",
      "Epoch 141, loss: 19471.110412\n",
      "Epoch 142, loss: 19468.537693\n",
      "Epoch 143, loss: 19465.051037\n",
      "Epoch 144, loss: 19461.907003\n",
      "Epoch 145, loss: 19459.203321\n",
      "Epoch 146, loss: 19456.163578\n",
      "Epoch 147, loss: 19453.878080\n",
      "Epoch 148, loss: 19451.333327\n",
      "Epoch 149, loss: 19447.628802\n",
      "Epoch 150, loss: 19445.111900\n",
      "Epoch 151, loss: 19442.229005\n",
      "Epoch 152, loss: 19439.704165\n",
      "Epoch 153, loss: 19436.812910\n",
      "Epoch 154, loss: 19434.144759\n",
      "Epoch 155, loss: 19431.599527\n",
      "Epoch 156, loss: 19429.133031\n",
      "Epoch 157, loss: 19425.878230\n",
      "Epoch 158, loss: 19423.217755\n",
      "Epoch 159, loss: 19420.974464\n",
      "Epoch 160, loss: 19417.971779\n",
      "Epoch 161, loss: 19415.910816\n",
      "Epoch 162, loss: 19413.403992\n",
      "Epoch 163, loss: 19410.810801\n",
      "Epoch 164, loss: 19408.408135\n",
      "Epoch 165, loss: 19405.753273\n",
      "Epoch 166, loss: 19403.279869\n",
      "Epoch 167, loss: 19400.941492\n",
      "Epoch 168, loss: 19398.754470\n",
      "Epoch 169, loss: 19396.610330\n",
      "Epoch 170, loss: 19393.814825\n",
      "Epoch 171, loss: 19391.516634\n",
      "Epoch 172, loss: 19388.751192\n",
      "Epoch 173, loss: 19386.942532\n",
      "Epoch 174, loss: 19384.405291\n",
      "Epoch 175, loss: 19382.673576\n",
      "Epoch 176, loss: 19379.423509\n",
      "Epoch 177, loss: 19377.558505\n",
      "Epoch 178, loss: 19375.044358\n",
      "Epoch 179, loss: 19373.156000\n",
      "Epoch 180, loss: 19371.261029\n",
      "Epoch 181, loss: 19369.206374\n",
      "Epoch 182, loss: 19366.488377\n",
      "Epoch 183, loss: 19364.529649\n",
      "Epoch 184, loss: 19362.611729\n",
      "Epoch 185, loss: 19360.479080\n",
      "Epoch 186, loss: 19357.786831\n",
      "Epoch 187, loss: 19355.667860\n",
      "Epoch 188, loss: 19354.073303\n",
      "Epoch 189, loss: 19351.795486\n",
      "Epoch 190, loss: 19349.559825\n",
      "Epoch 191, loss: 19347.623710\n",
      "Epoch 192, loss: 19345.531499\n",
      "Epoch 193, loss: 19343.247343\n",
      "Epoch 194, loss: 19341.112101\n",
      "Epoch 195, loss: 19339.835481\n",
      "Epoch 196, loss: 19337.254888\n",
      "Epoch 197, loss: 19335.432367\n",
      "Epoch 198, loss: 19333.411239\n",
      "Epoch 199, loss: 19331.497866\n",
      "1e-05 1e-06 0.229\n",
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg_str in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg_str)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        print(lr, reg_str, accuracy)\n",
    "            \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
