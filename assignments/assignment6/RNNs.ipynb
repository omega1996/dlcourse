{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==0.4.1 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch==0.4.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Omega\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Omega\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'X', 'DET', 'CONJ', 'ADP', 'NOUN', 'ADV', 'PRT', '.', 'ADJ', 'PRON', 'NUM', 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05434782608695652"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "total_count = mask.sum().item()\n",
    "\n",
    "correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.6893, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = model(X_batch)\n",
    "loss = 0\n",
    "for ind, row in enumerate(logits):\n",
    "    loss += criterion(row, y_batch[ind])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = 0\n",
    "                for ind, row in enumerate(logits):\n",
    "                    loss += criterion(row, y_batch[ind])\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                mask = (y_batch != 0).float()\n",
    "                \n",
    "                cur_correct_count, cur_sum_count = ((preds == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 19.25451, Accuracy = 72.31%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.31it/s]\n",
      "[1 / 50]   Val: Loss = 9.51296, Accuracy = 85.35%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 35.71it/s]\n",
      "[2 / 50] Train: Loss = 6.08354, Accuracy = 90.20%: 100%|█████████████████████████████| 572/572 [00:26<00:00, 21.23it/s]\n",
      "[2 / 50]   Val: Loss = 7.11997, Accuracy = 89.62%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 35.88it/s]\n",
      "[3 / 50] Train: Loss = 4.08192, Accuracy = 93.37%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.94it/s]\n",
      "[3 / 50]   Val: Loss = 6.31192, Accuracy = 91.35%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 34.57it/s]\n",
      "[4 / 50] Train: Loss = 3.07953, Accuracy = 94.94%: 100%|█████████████████████████████| 572/572 [00:28<00:00, 19.74it/s]\n",
      "[4 / 50]   Val: Loss = 5.89266, Accuracy = 92.19%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 35.03it/s]\n",
      "[5 / 50] Train: Loss = 2.44363, Accuracy = 95.95%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.96it/s]\n",
      "[5 / 50]   Val: Loss = 5.86592, Accuracy = 92.68%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 36.00it/s]\n",
      "[6 / 50] Train: Loss = 1.98629, Accuracy = 96.69%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 21.01it/s]\n",
      "[6 / 50]   Val: Loss = 6.01980, Accuracy = 92.89%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 34.38it/s]\n",
      "[7 / 50] Train: Loss = 1.63355, Accuracy = 97.26%: 100%|█████████████████████████████| 572/572 [00:26<00:00, 21.41it/s]\n",
      "[7 / 50]   Val: Loss = 6.07441, Accuracy = 93.02%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 33.33it/s]\n",
      "[8 / 50] Train: Loss = 1.34462, Accuracy = 97.74%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 21.06it/s]\n",
      "[8 / 50]   Val: Loss = 6.27441, Accuracy = 93.17%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 33.25it/s]\n",
      "[9 / 50] Train: Loss = 1.10986, Accuracy = 98.13%: 100%|█████████████████████████████| 572/572 [00:26<00:00, 21.31it/s]\n",
      "[9 / 50]   Val: Loss = 6.48353, Accuracy = 93.20%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 36.71it/s]\n",
      "[10 / 50] Train: Loss = 0.91473, Accuracy = 98.48%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.23it/s]\n",
      "[10 / 50]   Val: Loss = 6.61537, Accuracy = 93.24%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 36.41it/s]\n",
      "[11 / 50] Train: Loss = 0.75020, Accuracy = 98.77%: 100%|████████████████████████████| 572/572 [00:27<00:00, 21.18it/s]\n",
      "[11 / 50]   Val: Loss = 6.94849, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.21it/s]\n",
      "[12 / 50] Train: Loss = 0.61220, Accuracy = 99.01%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.45it/s]\n",
      "[12 / 50]   Val: Loss = 7.10482, Accuracy = 93.24%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.03it/s]\n",
      "[13 / 50] Train: Loss = 0.49195, Accuracy = 99.23%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.62it/s]\n",
      "[13 / 50]   Val: Loss = 7.58715, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.42it/s]\n",
      "[14 / 50] Train: Loss = 0.40268, Accuracy = 99.37%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.31it/s]\n",
      "[14 / 50]   Val: Loss = 7.98048, Accuracy = 93.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 31.32it/s]\n",
      "[15 / 50] Train: Loss = 0.32375, Accuracy = 99.51%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.54it/s]\n",
      "[15 / 50]   Val: Loss = 8.12350, Accuracy = 93.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.22it/s]\n",
      "[16 / 50] Train: Loss = 0.25634, Accuracy = 99.63%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.21it/s]\n",
      "[16 / 50]   Val: Loss = 8.52584, Accuracy = 93.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.56it/s]\n",
      "[17 / 50] Train: Loss = 0.21230, Accuracy = 99.69%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.36it/s]\n",
      "[17 / 50]   Val: Loss = 8.96257, Accuracy = 92.95%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.52it/s]\n",
      "[18 / 50] Train: Loss = 0.17979, Accuracy = 99.74%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.21it/s]\n",
      "[18 / 50]   Val: Loss = 9.24659, Accuracy = 92.95%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 36.41it/s]\n",
      "[19 / 50] Train: Loss = 0.15901, Accuracy = 99.77%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.87it/s]\n",
      "[19 / 50]   Val: Loss = 9.55567, Accuracy = 92.93%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.32it/s]\n",
      "[20 / 50] Train: Loss = 0.14193, Accuracy = 99.79%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.22it/s]\n",
      "[20 / 50]   Val: Loss = 10.02195, Accuracy = 92.90%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.90it/s]\n",
      "[21 / 50] Train: Loss = 0.12871, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.89it/s]\n",
      "[21 / 50]   Val: Loss = 10.26067, Accuracy = 92.95%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.03it/s]\n",
      "[22 / 50] Train: Loss = 0.12689, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.30it/s]\n",
      "[22 / 50]   Val: Loss = 10.47737, Accuracy = 92.94%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.82it/s]\n",
      "[23 / 50] Train: Loss = 0.12553, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:27<00:00, 21.02it/s]\n",
      "[23 / 50]   Val: Loss = 10.85556, Accuracy = 92.93%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.22it/s]\n",
      "[24 / 50] Train: Loss = 0.10194, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.91it/s]\n",
      "[24 / 50]   Val: Loss = 11.15574, Accuracy = 92.92%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.95it/s]\n",
      "[25 / 50] Train: Loss = 0.09911, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.23it/s]\n",
      "[25 / 50]   Val: Loss = 11.47787, Accuracy = 92.93%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.94it/s]\n",
      "[26 / 50] Train: Loss = 0.12296, Accuracy = 99.79%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.26it/s]\n",
      "[26 / 50]   Val: Loss = 11.51613, Accuracy = 92.88%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.84it/s]\n",
      "[27 / 50] Train: Loss = 0.11181, Accuracy = 99.81%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.24it/s]\n",
      "[27 / 50]   Val: Loss = 11.66540, Accuracy = 93.01%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.31it/s]\n",
      "[28 / 50] Train: Loss = 0.09448, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.21it/s]\n",
      "[28 / 50]   Val: Loss = 11.91389, Accuracy = 93.02%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.66it/s]\n",
      "[29 / 50] Train: Loss = 0.10249, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:27<00:00, 21.02it/s]\n",
      "[29 / 50]   Val: Loss = 12.31344, Accuracy = 92.84%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.51it/s]\n",
      "[30 / 50] Train: Loss = 0.10634, Accuracy = 99.81%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.93it/s]\n",
      "[30 / 50]   Val: Loss = 12.59094, Accuracy = 92.84%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.75it/s]\n",
      "[31 / 50] Train: Loss = 0.09324, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:27<00:00, 21.01it/s]\n",
      "[31 / 50]   Val: Loss = 12.74454, Accuracy = 93.04%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.32it/s]\n",
      "[32 / 50] Train: Loss = 0.08692, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.42it/s]\n",
      "[32 / 50]   Val: Loss = 12.91986, Accuracy = 93.00%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 32.99it/s]\n",
      "[33 / 50] Train: Loss = 0.08704, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.72it/s]\n",
      "[33 / 50]   Val: Loss = 13.03218, Accuracy = 93.03%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.71it/s]\n",
      "[34 / 50] Train: Loss = 0.08857, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.26it/s]\n",
      "[34 / 50]   Val: Loss = 13.11744, Accuracy = 92.99%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.41it/s]\n",
      "[35 / 50] Train: Loss = 0.10201, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.50it/s]\n",
      "[35 / 50]   Val: Loss = 13.35176, Accuracy = 92.92%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.75it/s]\n",
      "[36 / 50] Train: Loss = 0.11102, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.30it/s]\n",
      "[36 / 50]   Val: Loss = 13.41882, Accuracy = 92.96%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.71it/s]\n",
      "[37 / 50] Train: Loss = 0.08536, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.61it/s]\n",
      "[37 / 50]   Val: Loss = 13.65759, Accuracy = 93.05%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 37.24it/s]\n",
      "[38 / 50] Train: Loss = 0.08048, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.85it/s]\n",
      "[38 / 50]   Val: Loss = 13.79318, Accuracy = 93.02%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.21it/s]\n",
      "[39 / 50] Train: Loss = 0.07931, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.54it/s]\n",
      "[39 / 50]   Val: Loss = 14.04840, Accuracy = 92.99%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 38.00it/s]\n",
      "[40 / 50] Train: Loss = 0.07998, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.33it/s]\n",
      "[40 / 50]   Val: Loss = 14.08389, Accuracy = 92.98%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.82it/s]\n",
      "[41 / 50] Train: Loss = 0.08307, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.32it/s]\n",
      "[41 / 50]   Val: Loss = 14.13278, Accuracy = 92.98%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.00it/s]\n",
      "[42 / 50] Train: Loss = 0.12896, Accuracy = 99.76%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.57it/s]\n",
      "[42 / 50]   Val: Loss = 14.68114, Accuracy = 92.90%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 38.92it/s]\n",
      "[43 / 50] Train: Loss = 0.09154, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:27<00:00, 21.10it/s]\n",
      "[43 / 50]   Val: Loss = 14.69343, Accuracy = 93.04%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.30it/s]\n",
      "[44 / 50] Train: Loss = 0.07651, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.50it/s]\n",
      "[44 / 50]   Val: Loss = 14.78796, Accuracy = 93.03%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.93it/s]\n",
      "[45 / 50] Train: Loss = 0.07496, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.51it/s]\n",
      "[45 / 50]   Val: Loss = 15.08759, Accuracy = 93.05%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.61it/s]\n",
      "[46 / 50] Train: Loss = 0.07503, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.44it/s]\n",
      "[46 / 50]   Val: Loss = 15.17832, Accuracy = 93.05%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 37.68it/s]\n",
      "[47 / 50] Train: Loss = 0.07602, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.33it/s]\n",
      "[47 / 50]   Val: Loss = 15.37379, Accuracy = 93.05%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.50it/s]\n",
      "[48 / 50] Train: Loss = 0.07799, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.42it/s]\n",
      "[48 / 50]   Val: Loss = 15.62603, Accuracy = 92.97%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 37.56it/s]\n",
      "[49 / 50] Train: Loss = 0.13550, Accuracy = 99.75%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.81it/s]\n",
      "[49 / 50]   Val: Loss = 15.35966, Accuracy = 92.94%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 36.21it/s]\n",
      "[50 / 50] Train: Loss = 0.09600, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:26<00:00, 21.62it/s]\n",
      "[50 / 50]   Val: Loss = 15.38003, Accuracy = 93.05%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 37.67it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 93.14109250901129 %\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data, batch_size=64):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    val_accuracy = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        mask = (y_batch != 0).float()\n",
    "        \n",
    "        correct += ((pred == y_batch).float() * mask).sum().item()\n",
    "        \n",
    "        total += mask.sum().item()        \n",
    "        \n",
    "    val_accuracy = float(correct)/total\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional = True)\n",
    "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 20.98254, Accuracy = 88.78%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.80it/s]\n",
      "[1 / 50]   Val: Loss = 14.94529, Accuracy = 95.21%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 21.41it/s]\n",
      "[2 / 50] Train: Loss = 6.21927, Accuracy = 96.49%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.89it/s]\n",
      "[2 / 50]   Val: Loss = 12.95395, Accuracy = 95.77%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 21.95it/s]\n",
      "[3 / 50] Train: Loss = 3.85588, Accuracy = 97.51%: 100%|█████████████████████████████| 572/572 [00:37<00:00, 15.06it/s]\n",
      "[3 / 50]   Val: Loss = 11.66909, Accuracy = 95.99%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 23.19it/s]\n",
      "[4 / 50] Train: Loss = 3.32138, Accuracy = 97.79%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 15.03it/s]\n",
      "[4 / 50]   Val: Loss = 16.22068, Accuracy = 95.27%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 23.12it/s]\n",
      "[5 / 50] Train: Loss = 3.02298, Accuracy = 97.98%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.93it/s]\n",
      "[5 / 50]   Val: Loss = 14.73995, Accuracy = 95.53%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 23.05it/s]\n",
      "[6 / 50] Train: Loss = 2.74617, Accuracy = 98.14%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.90it/s]\n",
      "[6 / 50]   Val: Loss = 13.40622, Accuracy = 96.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 23.17it/s]\n",
      "[7 / 50] Train: Loss = 2.57820, Accuracy = 98.24%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.79it/s]\n",
      "[7 / 50]   Val: Loss = 15.89090, Accuracy = 95.23%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 22.14it/s]\n",
      "[8 / 50] Train: Loss = 2.48514, Accuracy = 98.34%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.86it/s]\n",
      "[8 / 50]   Val: Loss = 14.29327, Accuracy = 95.92%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 21.77it/s]\n",
      "[9 / 50] Train: Loss = 2.17249, Accuracy = 98.45%: 100%|█████████████████████████████| 572/572 [00:38<00:00, 14.85it/s]\n",
      "[9 / 50]   Val: Loss = 14.33243, Accuracy = 95.92%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 22.41it/s]\n",
      "[10 / 50] Train: Loss = 1.98281, Accuracy = 98.57%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.77it/s]\n",
      "[10 / 50]   Val: Loss = 19.19762, Accuracy = 95.17%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.45it/s]\n",
      "[11 / 50] Train: Loss = 2.05976, Accuracy = 98.53%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.82it/s]\n",
      "[11 / 50]   Val: Loss = 14.79574, Accuracy = 96.40%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.73it/s]\n",
      "[12 / 50] Train: Loss = 1.99285, Accuracy = 98.60%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.80it/s]\n",
      "[12 / 50]   Val: Loss = 16.99098, Accuracy = 95.60%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.63it/s]\n",
      "[13 / 50] Train: Loss = 1.73945, Accuracy = 98.71%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.94it/s]\n",
      "[13 / 50]   Val: Loss = 16.19848, Accuracy = 95.53%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.63it/s]\n",
      "[14 / 50] Train: Loss = 1.62814, Accuracy = 98.77%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.72it/s]\n",
      "[14 / 50]   Val: Loss = 13.89063, Accuracy = 96.28%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.10it/s]\n",
      "[15 / 50] Train: Loss = 1.64587, Accuracy = 98.76%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.86it/s]\n",
      "[15 / 50]   Val: Loss = 18.95662, Accuracy = 95.50%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.60it/s]\n",
      "[16 / 50] Train: Loss = 1.85591, Accuracy = 98.70%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.83it/s]\n",
      "[16 / 50]   Val: Loss = 16.41572, Accuracy = 96.04%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.45it/s]\n",
      "[17 / 50] Train: Loss = 1.45493, Accuracy = 98.89%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.80it/s]\n",
      "[17 / 50]   Val: Loss = 14.47314, Accuracy = 96.07%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.47it/s]\n",
      "[18 / 50] Train: Loss = 1.49367, Accuracy = 98.88%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.79it/s]\n",
      "[18 / 50]   Val: Loss = 14.97082, Accuracy = 96.39%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 20.73it/s]\n",
      "[19 / 50] Train: Loss = 1.42213, Accuracy = 98.90%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.81it/s]\n",
      "[19 / 50]   Val: Loss = 14.34242, Accuracy = 96.60%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.63it/s]\n",
      "[20 / 50] Train: Loss = 1.31739, Accuracy = 98.95%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.93it/s]\n",
      "[20 / 50]   Val: Loss = 12.64216, Accuracy = 96.57%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.11it/s]\n",
      "[21 / 50] Train: Loss = 1.66956, Accuracy = 98.79%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.97it/s]\n",
      "[21 / 50]   Val: Loss = 12.61077, Accuracy = 96.78%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.38it/s]\n",
      "[22 / 50] Train: Loss = 1.40178, Accuracy = 98.96%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.79it/s]\n",
      "[22 / 50]   Val: Loss = 13.14541, Accuracy = 96.88%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.92it/s]\n",
      "[23 / 50] Train: Loss = 1.32437, Accuracy = 98.99%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.93it/s]\n",
      "[23 / 50]   Val: Loss = 11.05215, Accuracy = 96.86%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.59it/s]\n",
      "[24 / 50] Train: Loss = 1.20891, Accuracy = 99.05%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.85it/s]\n",
      "[24 / 50]   Val: Loss = 17.48059, Accuracy = 96.21%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.41it/s]\n",
      "[25 / 50] Train: Loss = 1.41820, Accuracy = 98.93%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.92it/s]\n",
      "[25 / 50]   Val: Loss = 15.30385, Accuracy = 96.72%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.07it/s]\n",
      "[26 / 50] Train: Loss = 1.38883, Accuracy = 98.94%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.88it/s]\n",
      "[26 / 50]   Val: Loss = 15.11575, Accuracy = 96.47%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.09it/s]\n",
      "[27 / 50] Train: Loss = 1.23711, Accuracy = 99.05%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.77it/s]\n",
      "[27 / 50]   Val: Loss = 19.73141, Accuracy = 95.74%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.80it/s]\n",
      "[28 / 50] Train: Loss = 1.11120, Accuracy = 99.10%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.70it/s]\n",
      "[28 / 50]   Val: Loss = 15.09508, Accuracy = 96.65%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.68it/s]\n",
      "[29 / 50] Train: Loss = 1.18093, Accuracy = 99.03%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.83it/s]\n",
      "[29 / 50]   Val: Loss = 18.19844, Accuracy = 96.00%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.92it/s]\n",
      "[30 / 50] Train: Loss = 1.20696, Accuracy = 99.06%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.83it/s]\n",
      "[30 / 50]   Val: Loss = 23.06338, Accuracy = 95.39%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.16it/s]\n",
      "[31 / 50] Train: Loss = 1.04372, Accuracy = 99.14%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.95it/s]\n",
      "[31 / 50]   Val: Loss = 20.42057, Accuracy = 95.35%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.81it/s]\n",
      "[32 / 50] Train: Loss = 1.31261, Accuracy = 99.00%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.98it/s]\n",
      "[32 / 50]   Val: Loss = 14.89583, Accuracy = 96.73%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.10it/s]\n",
      "[33 / 50] Train: Loss = 1.20460, Accuracy = 99.06%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.77it/s]\n",
      "[33 / 50]   Val: Loss = 21.56671, Accuracy = 95.41%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.76it/s]\n",
      "[34 / 50] Train: Loss = 1.18973, Accuracy = 99.06%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.70it/s]\n",
      "[34 / 50]   Val: Loss = 22.30723, Accuracy = 95.67%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.43it/s]\n",
      "[35 / 50] Train: Loss = 1.12771, Accuracy = 99.09%: 100%|████████████████████████████| 572/572 [00:39<00:00, 14.66it/s]\n",
      "[35 / 50]   Val: Loss = 20.72617, Accuracy = 95.71%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.70it/s]\n",
      "[36 / 50] Train: Loss = 1.22406, Accuracy = 99.05%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.85it/s]\n",
      "[36 / 50]   Val: Loss = 22.44494, Accuracy = 95.64%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.88it/s]\n",
      "[37 / 50] Train: Loss = 1.19362, Accuracy = 99.09%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.79it/s]\n",
      "[37 / 50]   Val: Loss = 17.08851, Accuracy = 96.47%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.10it/s]\n",
      "[38 / 50] Train: Loss = 1.08337, Accuracy = 99.12%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.83it/s]\n",
      "[38 / 50]   Val: Loss = 19.69309, Accuracy = 95.55%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.80it/s]\n",
      "[39 / 50] Train: Loss = 1.05765, Accuracy = 99.15%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.75it/s]\n",
      "[39 / 50]   Val: Loss = 18.97436, Accuracy = 95.78%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.64it/s]\n",
      "[40 / 50] Train: Loss = 1.06598, Accuracy = 99.16%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.75it/s]\n",
      "[40 / 50]   Val: Loss = 28.22037, Accuracy = 95.18%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.59it/s]\n",
      "[41 / 50] Train: Loss = 1.11554, Accuracy = 99.10%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.83it/s]\n",
      "[41 / 50]   Val: Loss = 25.40653, Accuracy = 95.27%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.29it/s]\n",
      "[42 / 50] Train: Loss = 1.04299, Accuracy = 99.14%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.78it/s]\n",
      "[42 / 50]   Val: Loss = 23.22625, Accuracy = 95.55%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.33it/s]\n",
      "[43 / 50] Train: Loss = 1.05836, Accuracy = 99.16%: 100%|████████████████████████████| 572/572 [00:39<00:00, 14.61it/s]\n",
      "[43 / 50]   Val: Loss = 24.88743, Accuracy = 95.20%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.41it/s]\n",
      "[44 / 50] Train: Loss = 1.17527, Accuracy = 99.08%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.84it/s]\n",
      "[44 / 50]   Val: Loss = 21.28276, Accuracy = 95.51%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.53it/s]\n",
      "[45 / 50] Train: Loss = 1.08417, Accuracy = 99.14%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.86it/s]\n",
      "[45 / 50]   Val: Loss = 19.22597, Accuracy = 95.54%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.80it/s]\n",
      "[46 / 50] Train: Loss = 1.00910, Accuracy = 99.19%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.81it/s]\n",
      "[46 / 50]   Val: Loss = 17.46465, Accuracy = 96.13%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.56it/s]\n",
      "[47 / 50] Train: Loss = 0.96336, Accuracy = 99.20%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.77it/s]\n",
      "[47 / 50]   Val: Loss = 18.89713, Accuracy = 95.67%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 23.04it/s]\n",
      "[48 / 50] Train: Loss = 1.11038, Accuracy = 99.11%: 100%|████████████████████████████| 572/572 [00:39<00:00, 14.66it/s]\n",
      "[48 / 50]   Val: Loss = 22.58151, Accuracy = 95.45%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 21.60it/s]\n",
      "[49 / 50] Train: Loss = 1.10968, Accuracy = 99.13%: 100%|████████████████████████████| 572/572 [00:39<00:00, 14.61it/s]\n",
      "[49 / 50]   Val: Loss = 29.75653, Accuracy = 94.95%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.74it/s]\n",
      "[50 / 50] Train: Loss = 1.19071, Accuracy = 99.08%: 100%|████████████████████████████| 572/572 [00:38<00:00, 14.89it/s]\n",
      "[50 / 50]   Val: Loss = 26.70227, Accuracy = 95.26%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 22.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    lstm_layers_count=2,\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM is 95.34717990560094 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_t = Variable(torch.from_numpy(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_t = embeddings_t.float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding.from_pretrained(embeddings_t)\n",
    "        self._lstm = nn.LSTM(embeddings_t.shape[1], \n",
    "                             lstm_hidden_dim, \n",
    "                             num_layers=lstm_layers_count, \n",
    "                             bidirectional = True)\n",
    "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 44.10250, Accuracy = 79.51%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.38it/s]\n",
      "[1 / 50]   Val: Loss = 35.01377, Accuracy = 89.61%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.19it/s]\n",
      "[2 / 50] Train: Loss = 16.62921, Accuracy = 91.75%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.78it/s]\n",
      "[2 / 50]   Val: Loss = 25.64155, Accuracy = 92.35%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.48it/s]\n",
      "[3 / 50] Train: Loss = 12.43132, Accuracy = 93.75%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.73it/s]\n",
      "[3 / 50]   Val: Loss = 18.43486, Accuracy = 93.68%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 37.03it/s]\n",
      "[4 / 50] Train: Loss = 9.92870, Accuracy = 94.81%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.88it/s]\n",
      "[4 / 50]   Val: Loss = 16.63602, Accuracy = 94.38%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.81it/s]\n",
      "[5 / 50] Train: Loss = 8.33551, Accuracy = 95.48%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.98it/s]\n",
      "[5 / 50]   Val: Loss = 16.11280, Accuracy = 94.90%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.99it/s]\n",
      "[6 / 50] Train: Loss = 7.15019, Accuracy = 96.00%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.80it/s]\n",
      "[6 / 50]   Val: Loss = 13.50731, Accuracy = 95.25%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 36.61it/s]\n",
      "[7 / 50] Train: Loss = 6.25099, Accuracy = 96.33%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.97it/s]\n",
      "[7 / 50]   Val: Loss = 13.13397, Accuracy = 95.67%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.41it/s]\n",
      "[8 / 50] Train: Loss = 5.63256, Accuracy = 96.62%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.88it/s]\n",
      "[8 / 50]   Val: Loss = 12.77202, Accuracy = 95.71%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.57it/s]\n",
      "[9 / 50] Train: Loss = 5.01437, Accuracy = 96.82%: 100%|█████████████████████████████| 572/572 [00:27<00:00, 20.82it/s]\n",
      "[9 / 50]   Val: Loss = 10.74233, Accuracy = 96.00%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.51it/s]\n",
      "[10 / 50] Train: Loss = 4.51389, Accuracy = 97.02%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.65it/s]\n",
      "[10 / 50]   Val: Loss = 11.64498, Accuracy = 95.99%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.80it/s]\n",
      "[11 / 50] Train: Loss = 4.19900, Accuracy = 97.15%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.76it/s]\n",
      "[11 / 50]   Val: Loss = 10.57107, Accuracy = 96.27%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.22it/s]\n",
      "[12 / 50] Train: Loss = 3.84358, Accuracy = 97.32%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.64it/s]\n",
      "[12 / 50]   Val: Loss = 11.14663, Accuracy = 96.28%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.58it/s]\n",
      "[13 / 50] Train: Loss = 3.60029, Accuracy = 97.42%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.75it/s]\n",
      "[13 / 50]   Val: Loss = 10.65143, Accuracy = 96.41%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.51it/s]\n",
      "[14 / 50] Train: Loss = 3.32296, Accuracy = 97.55%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.82it/s]\n",
      "[14 / 50]   Val: Loss = 10.72006, Accuracy = 96.47%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.85it/s]\n",
      "[15 / 50] Train: Loss = 3.11873, Accuracy = 97.66%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.93it/s]\n",
      "[15 / 50]   Val: Loss = 9.97151, Accuracy = 96.50%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.33it/s]\n",
      "[16 / 50] Train: Loss = 2.92842, Accuracy = 97.74%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.71it/s]\n",
      "[16 / 50]   Val: Loss = 9.53459, Accuracy = 96.58%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.60it/s]\n",
      "[17 / 50] Train: Loss = 2.72724, Accuracy = 97.86%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.46it/s]\n",
      "[17 / 50]   Val: Loss = 11.48772, Accuracy = 96.55%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 32.33it/s]\n",
      "[18 / 50] Train: Loss = 2.63475, Accuracy = 97.91%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.73it/s]\n",
      "[18 / 50]   Val: Loss = 10.33108, Accuracy = 96.52%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.35it/s]\n",
      "[19 / 50] Train: Loss = 2.46858, Accuracy = 97.99%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.77it/s]\n",
      "[19 / 50]   Val: Loss = 10.11486, Accuracy = 96.66%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.57it/s]\n",
      "[20 / 50] Train: Loss = 2.32484, Accuracy = 98.06%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.91it/s]\n",
      "[20 / 50]   Val: Loss = 10.72120, Accuracy = 96.70%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.51it/s]\n",
      "[21 / 50] Train: Loss = 2.19509, Accuracy = 98.15%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.50it/s]\n",
      "[21 / 50]   Val: Loss = 9.78796, Accuracy = 96.70%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.11it/s]\n",
      "[22 / 50] Train: Loss = 2.10593, Accuracy = 98.19%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.72it/s]\n",
      "[22 / 50]   Val: Loss = 9.97088, Accuracy = 96.74%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.58it/s]\n",
      "[23 / 50] Train: Loss = 1.99413, Accuracy = 98.27%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.65it/s]\n",
      "[23 / 50]   Val: Loss = 9.41402, Accuracy = 96.71%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 34.84it/s]\n",
      "[24 / 50] Train: Loss = 1.90982, Accuracy = 98.32%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.43it/s]\n",
      "[24 / 50]   Val: Loss = 10.93443, Accuracy = 96.73%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.58it/s]\n",
      "[25 / 50] Train: Loss = 1.81337, Accuracy = 98.38%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.27it/s]\n",
      "[25 / 50]   Val: Loss = 10.11465, Accuracy = 96.75%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.41it/s]\n",
      "[26 / 50] Train: Loss = 1.71976, Accuracy = 98.44%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.36it/s]\n",
      "[26 / 50]   Val: Loss = 10.61282, Accuracy = 96.76%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.48it/s]\n",
      "[27 / 50] Train: Loss = 1.64340, Accuracy = 98.50%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.39it/s]\n",
      "[27 / 50]   Val: Loss = 10.46633, Accuracy = 96.79%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.75it/s]\n",
      "[28 / 50] Train: Loss = 1.57074, Accuracy = 98.55%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.08it/s]\n",
      "[28 / 50]   Val: Loss = 11.38049, Accuracy = 96.80%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.02it/s]\n",
      "[29 / 50] Train: Loss = 1.49006, Accuracy = 98.61%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.24it/s]\n",
      "[29 / 50]   Val: Loss = 11.21404, Accuracy = 96.74%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 31.93it/s]\n",
      "[30 / 50] Train: Loss = 1.43675, Accuracy = 98.66%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.37it/s]\n",
      "[30 / 50]   Val: Loss = 10.55599, Accuracy = 96.83%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.88it/s]\n",
      "[31 / 50] Train: Loss = 1.36768, Accuracy = 98.70%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.63it/s]\n",
      "[31 / 50]   Val: Loss = 10.59756, Accuracy = 96.80%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.41it/s]\n",
      "[32 / 50] Train: Loss = 1.29659, Accuracy = 98.76%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.26it/s]\n",
      "[32 / 50]   Val: Loss = 11.31982, Accuracy = 96.84%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.41it/s]\n",
      "[33 / 50] Train: Loss = 1.23583, Accuracy = 98.80%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.20it/s]\n",
      "[33 / 50]   Val: Loss = 9.69328, Accuracy = 96.80%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.32it/s]\n",
      "[34 / 50] Train: Loss = 1.18346, Accuracy = 98.86%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.49it/s]\n",
      "[34 / 50]   Val: Loss = 10.52150, Accuracy = 96.78%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.32it/s]\n",
      "[35 / 50] Train: Loss = 1.11009, Accuracy = 98.91%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.02it/s]\n",
      "[35 / 50]   Val: Loss = 10.93125, Accuracy = 96.76%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.21it/s]\n",
      "[36 / 50] Train: Loss = 1.07423, Accuracy = 98.95%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.50it/s]\n",
      "[36 / 50]   Val: Loss = 11.06865, Accuracy = 96.79%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 35.80it/s]\n",
      "[37 / 50] Train: Loss = 1.01795, Accuracy = 98.99%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.41it/s]\n",
      "[37 / 50]   Val: Loss = 11.28925, Accuracy = 96.75%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.75it/s]\n",
      "[38 / 50] Train: Loss = 0.98114, Accuracy = 99.03%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.48it/s]\n",
      "[38 / 50]   Val: Loss = 11.77734, Accuracy = 96.80%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.41it/s]\n",
      "[39 / 50] Train: Loss = 0.93014, Accuracy = 99.08%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.66it/s]\n",
      "[39 / 50]   Val: Loss = 13.58925, Accuracy = 96.78%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.50it/s]\n",
      "[40 / 50] Train: Loss = 0.89424, Accuracy = 99.11%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.30it/s]\n",
      "[40 / 50]   Val: Loss = 11.64624, Accuracy = 96.80%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.94it/s]\n",
      "[41 / 50] Train: Loss = 0.85372, Accuracy = 99.15%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.85it/s]\n",
      "[41 / 50]   Val: Loss = 13.05943, Accuracy = 96.68%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.31it/s]\n",
      "[42 / 50] Train: Loss = 0.81902, Accuracy = 99.17%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.24it/s]\n",
      "[42 / 50]   Val: Loss = 12.10563, Accuracy = 96.78%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.76it/s]\n",
      "[43 / 50] Train: Loss = 0.77359, Accuracy = 99.21%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.48it/s]\n",
      "[43 / 50]   Val: Loss = 12.74652, Accuracy = 96.77%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.38it/s]\n",
      "[44 / 50] Train: Loss = 0.72707, Accuracy = 99.27%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.45it/s]\n",
      "[44 / 50]   Val: Loss = 10.97673, Accuracy = 96.72%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.50it/s]\n",
      "[45 / 50] Train: Loss = 0.70146, Accuracy = 99.30%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.38it/s]\n",
      "[45 / 50]   Val: Loss = 12.30599, Accuracy = 96.67%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.20it/s]\n",
      "[46 / 50] Train: Loss = 0.66437, Accuracy = 99.34%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.50it/s]\n",
      "[46 / 50]   Val: Loss = 11.82727, Accuracy = 96.66%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.93it/s]\n",
      "[47 / 50] Train: Loss = 0.64210, Accuracy = 99.35%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.43it/s]\n",
      "[47 / 50]   Val: Loss = 12.85161, Accuracy = 96.63%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.85it/s]\n",
      "[48 / 50] Train: Loss = 0.60265, Accuracy = 99.39%: 100%|████████████████████████████| 572/572 [00:28<00:00, 20.33it/s]\n",
      "[48 / 50]   Val: Loss = 12.66491, Accuracy = 96.69%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 33.76it/s]\n",
      "[49 / 50] Train: Loss = 0.58555, Accuracy = 99.41%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.44it/s]\n",
      "[49 / 50]   Val: Loss = 14.09276, Accuracy = 96.63%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.53it/s]\n",
      "[50 / 50] Train: Loss = 0.55718, Accuracy = 99.45%: 100%|████████████████████████████| 572/572 [00:27<00:00, 20.53it/s]\n",
      "[50 / 50]   Val: Loss = 13.60526, Accuracy = 96.59%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 34.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM with pretrained embeddings is 96.57278401481742 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM with pretrained embeddings is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
